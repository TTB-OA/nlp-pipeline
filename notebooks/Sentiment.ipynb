{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b5270d-4b3b-4163-b03a-81cb1940f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from typing import List, Dict, Any\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf899e42-da9f-45ed-9419-2570a43bd1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docket_id = \"TTB-2025-0003\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "097401b1-9607-402b-b7b3-57b126c3e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo / outputs / model\n",
    "try:\n",
    "    REPO_ROOT = Path(__file__).parent.parent.resolve()\n",
    "except NameError:\n",
    "    REPO_ROOT = Path(os.getcwd()).parent.resolve()\n",
    "\n",
    "OUTPUTS_DIR = REPO_ROOT / \"outputs\"\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "COMMENTS_CSV = OUTPUTS_DIR / f\"comments_with_bertopic_{docket_id}.csv\"\n",
    "TOPIC_SUMMARY_CSV = OUTPUTS_DIR / f\"bertopic_topic_summary_{docket_id}.csv\"\n",
    "\n",
    "comments_df = pd.read_csv(COMMENTS_CSV)\n",
    "topic_summary = pd.read_csv(TOPIC_SUMMARY_CSV)\n",
    "\n",
    "# print(\"Comments:\", len(comments_df))\n",
    "# print(\"Topic summary rows:\", len(topic_summary))\n",
    "# comments_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca842ad3-7b6b-4660-af88-8e4d2f5acdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linna\\OneDrive\\Documents\\Python_Dev\\topic-modeling\\outputs\\comments_with_bertopic_TTB-2025-0003.csv\n"
     ]
    }
   ],
   "source": [
    "print(COMMENTS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd4dcc0a-9452-48e4-9588-0eff2ad34f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# params #\n",
    "##########\n",
    "\n",
    "MODEL_NAME = \"SamLowe/roberta-base-go_emotions\"  #huggingface.co for more\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1  # use GPU if available\n",
    "MAX_LENGTH = 512   \n",
    "CHUNK_LONG_TEXTS = True   # set False to simply truncate long comments\n",
    "CHUNK_SIZE_WORDS = 300    # approximate chunk size in words (must be <= MAX_LENGTH tokens)\n",
    "CHUNK_OVERLAP_WORDS = 50  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8b1631a-c272-40b6-bf44-118a190ee9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_chunk_text(text: str, chunk_size: int = CHUNK_SIZE_WORDS, overlap: int = CHUNK_OVERLAP_WORDS):\n",
    "    toks = text.split()\n",
    "    if len(toks) <= chunk_size:\n",
    "        return [text]\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    L = len(toks)\n",
    "    while start < L:\n",
    "        end = min(L, start + chunk_size)\n",
    "        chunks.append(\" \".join(toks[start:end]))\n",
    "        if end == L:\n",
    "            break\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def classify_texts(texts: List[str], batch_size:int = BATCH_SIZE, chunk_long: bool = CHUNK_LONG_TEXTS):\n",
    "    \"\"\"\n",
    "    Returns list of (top_label, top_score, full_scores_dict) per original text.\n",
    "    If chunk_long==True, splits long texts into chunks and aggregates by max score across chunks.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        # For each text, create list of chunks (1 element if not chunking)\n",
    "        doc_chunks = [word_chunk_text(t) if (chunk_long and len(t.split()) > CHUNK_SIZE_WORDS) else [t] for t in batch]\n",
    "        # flatten chunks for batching\n",
    "        flat_chunks = [c for sub in doc_chunks for c in sub]\n",
    "        # tokenize with truncation and padding\n",
    "        enc = tokenizer(flat_chunks, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "        enc = {k: v.to(\"cuda\" if DEVICE != -1 else \"cpu\") for k, v in enc.items()}\n",
    "        with torch.no_grad():\n",
    "            out = model(**enc)\n",
    "            logits = out.logits.detach().cpu().numpy()  # shape (n_chunks, n_labels)\n",
    "\n",
    "        # split logits back into per-doc lists\n",
    "        idx = 0\n",
    "        for chunks in doc_chunks:\n",
    "            n = len(chunks)\n",
    "            if n == 1:\n",
    "                log = logits[idx]\n",
    "                probs = softmax(log)\n",
    "                best_idx = int(np.argmax(probs))\n",
    "                label = id2label[best_idx] if id2label is not None else str(best_idx)\n",
    "                results.append((label, float(probs[best_idx]), {id2label[j] if id2label else str(j): float(probs[j]) for j in range(len(probs))}))\n",
    "            else:\n",
    "                # aggregate across chunks (take max probability per label)\n",
    "                chunk_logits = logits[idx: idx + n]  # shape (n, n_labels)\n",
    "                chunk_probs = softmax(chunk_logits)  # shape (n, n_labels)\n",
    "                # aggregate by max across chunks (keeps strong signals)\n",
    "                agg = chunk_probs.max(axis=0)  # shape (n_labels,)\n",
    "                best_idx = int(np.argmax(agg))\n",
    "                label = id2label[best_idx] if id2label is not None else str(best_idx)\n",
    "                results.append((label, float(agg[best_idx]), {id2label[j] if id2label else str(j): float(agg[j]) for j in range(len(agg))}))\n",
    "            idx += n\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c36edca9-ec20-418c-92f2-91070871c51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.to(\"cuda\" if DEVICE != -1 else \"cpu\")\n",
    "id2label = {int(k): v for k, v in model.config.id2label.items()} if hasattr(model.config, \"id2label\") else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad761db3-3cf6-46a5-b346-d9aa0c3484fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying 214 comments (device) cpu\n"
     ]
    }
   ],
   "source": [
    "texts = comments_df[\"comment_text\"].astype(str).tolist()\n",
    "print(\"Classifying\", len(texts), \"comments (device)\", \"cuda\" if DEVICE!=-1 else \"cpu\")\n",
    "out = classify_texts(texts, batch_size=BATCH_SIZE, chunk_long=CHUNK_LONG_TEXTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7f7456c-b55b-4468-9f26-5997bf43814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach to dataframe\n",
    "top_labels, top_scores, full_scores = zip(*out)\n",
    "comments_df[\"top_emotion\"] = top_labels\n",
    "comments_df[\"top_emotion_score\"] = top_scores\n",
    "comments_df[\"top_emotion_scores_full\"] = full_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "589f8440-c205-41d9-8863-a2b734a1ace2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Sample:\n",
      "        comment_id top_emotion  top_emotion_score\n",
      "TTB-2025-0003-2168   gratitude           0.987184\n",
      "TTB-2025-0003-2630   gratitude           0.955423\n",
      "TTB-2025-0003-2632     neutral           0.997809\n",
      "TTB-2025-0003-1124   gratitude           0.986016\n",
      "TTB-2025-0003-0161     neutral           0.600973\n"
     ]
    }
   ],
   "source": [
    "print(\"Done. Sample:\")\n",
    "print(comments_df[[\"comment_id\",\"top_emotion\",\"top_emotion_score\"]].head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab2596f6-3a21-495f-bac0-cc4c24ed4790",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.to_csv(COMMENTS_CSV, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
