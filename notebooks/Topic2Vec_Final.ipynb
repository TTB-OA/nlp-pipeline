{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb902889-dad8-442b-a12b-2ebcb454060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from top2vec import Top2Vec\n",
    "\n",
    "##############\n",
    "# parameters #\n",
    "##############\n",
    "\n",
    "DATA_PATH = Path(\"C:\\\\Users\\\\linna\\\\OneDrive\\\\Documents\\\\Python_Dev\\\\topic-modeling\\\\data\\\\public_comments.json\")\n",
    "\n",
    "TEXT_COL = \"comment_text\"\n",
    "DOC_ID_COL = \"comment_id\"\n",
    "DOCKET_TO_USE = \"TTB-2025-0003\"   # change as needed (or set to None to use full df)\n",
    "\n",
    "# will clean later:\n",
    "try:\n",
    "    REPO_ROOT = Path(__file__).parent.parent.resolve() # keep this for .py\n",
    "except NameError:\n",
    "    # fallback for notebook\n",
    "    REPO_ROOT = Path(os.getcwd()).parent.resolve()\n",
    "\n",
    "OUTPUTS_DIR = REPO_ROOT / \"outputs\"\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# OUTPUTS_DIR = Path(\"outputs\")\n",
    "# OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TOPIC_SUMMARY_CSV = OUTPUTS_DIR / \"top2vec_topic_summary.csv\"\n",
    "OUTPUT_DF_CSV = OUTPUTS_DIR / \"comments_with_top2vec.csv\"\n",
    "\n",
    "# TOPIC_SUMMARY_CSV = Path(\"top2vec_topic_summary.csv\")\n",
    "# OUTPUT_DF_CSV = Path(\"comments_with_top2vec.csv\")\n",
    "# MODEL_SAVE_DIR = Path(\"top2vec_model_classic\")\n",
    "\n",
    "# top2vec training\n",
    "EMBEDDING_MODEL = \"doc2vec\" # changed from originial all-MiniLM-L6-v2 for richer embeddings \n",
    "SPEED = \"deep-learn\"        # options are: 'fast-learn', 'learn', 'deep-learn' \n",
    "WORKERS = os.cpu_count() or 1\n",
    "\n",
    "# output params\n",
    "TOP_WORDS_PER_TOPIC = 10\n",
    "SAMPLE_DOCS_PER_TOPIC = 5\n",
    "TOP_N_FOR_LABEL = 5    # how many top words to use to create a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f726d2f-c276-47e8-a971-6c849aa09c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_FILE = Path(r\"C:\\Users\\linna\\Documents\\top2vec_model_classic.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2c54ad-2a97-42d1-8912-ab1b2bd98078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_json(path, orient=\"records\", lines=False)\n",
    "\n",
    "    if TEXT_COL not in df.columns:\n",
    "        raise ValueError(f\"{TEXT_COL} not found in dataframe columns: {df.columns.tolist()}\")\n",
    "\n",
    "    if \"comment_title\" in df.columns:\n",
    "        # deduplicate mass comments\n",
    "        pattern = re.compile(r'^\\s*Mass Comment\\s*[#\\(\\-:\\s]*\\s*(\\d+)', flags=re.IGNORECASE)\n",
    "\n",
    "        def _extract_mass_num(title):\n",
    "            if not isinstance(title, str):\n",
    "                return None\n",
    "            m = pattern.match(title)\n",
    "            if m:\n",
    "                try:\n",
    "                    return int(m.group(1))\n",
    "                except ValueError:\n",
    "                    return None\n",
    "            return None\n",
    "\n",
    "        df[\"__mass_num\"] = df[\"comment_title\"].apply(_extract_mass_num)\n",
    "\n",
    "        # keep first occurrence for each mass_num, drop subsequent ones\n",
    "        mask_mass = df[\"__mass_num\"].notna()\n",
    "        before_len = len(df)\n",
    "        duplicated_mask = df.loc[mask_mass, \"__mass_num\"].duplicated(keep=\"first\")\n",
    "        dup_index = df.loc[mask_mass].index[duplicated_mask]\n",
    "        if len(dup_index) > 0:\n",
    "            df = df.drop(index=dup_index).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.reset_index(drop=True)\n",
    "        after_len = len(df)\n",
    "        print(f\"Dropped {before_len - after_len} duplicate 'Mass Comment N' rows (kept first of each N).\")\n",
    "\n",
    "        df = df.drop(columns=\"__mass_num\")\n",
    "        \n",
    "    df = df.dropna(subset=[TEXT_COL]).reset_index(drop=True)\n",
    "    print(f\"Loaded {len(df)} comments from {path}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_by_docket(df: pd.DataFrame, docket: str | None) -> pd.DataFrame:\n",
    "    if docket is None:\n",
    "        return df\n",
    "    if \"docket_id\" not in df.columns:\n",
    "        raise ValueError(\"docket_id column not in dataframe\")\n",
    "    df_sub = df[df[\"docket_id\"] == docket].reset_index(drop=True)\n",
    "    print(f\"Filtered to docket '{docket}': {len(df_sub)} comments\")\n",
    "    return df_sub\n",
    "\n",
    "\n",
    "def train_top2vec(\n",
    "    documents: List[str],\n",
    "    document_ids: List[str],\n",
    "    embedding_model: str,\n",
    "    speed: str,\n",
    "    workers: int\n",
    ") -> Top2Vec:\n",
    "    print(\"Training Top2Vec:\")\n",
    "    print(f\"  embedding_model={embedding_model}, speed={speed}, workers={workers}\")\n",
    "    model = Top2Vec(\n",
    "        documents=documents,\n",
    "        document_ids=document_ids,\n",
    "        embedding_model=embedding_model,\n",
    "        speed=speed,\n",
    "        workers=workers\n",
    "    )\n",
    "    print(\"Training complete.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def map_dominant_and_topN(model: Top2Vec, df: pd.DataFrame, doc_id_col: str, text_col: str, topN: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds columns:\n",
    "      - top2vec_dominant_topic (int, -1 if missing)\n",
    "      - top2vec_top_topics (list of ints, length topN)\n",
    "    \"\"\"\n",
    "    df_ids_str = df[doc_id_col].astype(str).tolist()\n",
    "\n",
    "    model_doc_type = getattr(model, \"doc_id_type\", None)\n",
    "    if model_doc_type is None:\n",
    "        try:\n",
    "            model_doc_type = type(model.document_ids[0])\n",
    "        except Exception:\n",
    "            model_doc_type = str\n",
    "\n",
    "    def to_model_type(x):\n",
    "        try:\n",
    "            return model_doc_type(x)\n",
    "        except Exception:\n",
    "            return x\n",
    "\n",
    "    coerced_ids = [to_model_type(x) for x in df_ids_str]\n",
    "    try:\n",
    "        topn_res = model.get_documents_topics(coerced_ids, num_topics=topN)\n",
    "        # import numpy as _np\n",
    "        arr = np.array(topn_res[0]) if isinstance(topn_res, (list, tuple)) and len(topn_res) >= 1 else np.array(topn_res)\n",
    "        # if arr is shape (n_docs, topN)\n",
    "        df[\"top2vec_top_topics\"] = list(arr.tolist())\n",
    "        df[\"top2vec_dominant_topic\"] = arr[:, 0].astype(int)\n",
    "        for i in range(min(topN, arr.shape[1])):\n",
    "            df[f\"top2vec_topic_rank_{i+1}\"] = arr[:, i].astype(int)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"get_documents_topics failed; falling back to search_documents_by_topic iterative mapping.\")\n",
    "        print(\"Exception:\", repr(e))\n",
    "\n",
    "    docid_to_topic = {}\n",
    "    try:\n",
    "        topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "    except Exception:\n",
    "        try:\n",
    "            n_topics = model.get_num_topics()\n",
    "            topic_nums = list(range(n_topics))\n",
    "            topic_sizes = [0] * n_topics\n",
    "        except Exception:\n",
    "            raise RuntimeError(\"Cannot obtain topic list from model for fallback mapping.\")\n",
    "\n",
    "    for size, tnum in zip(topic_sizes, topic_nums):\n",
    "        num_to_get = min(max(1, int(size)), len(df_ids_str))\n",
    "        try:\n",
    "            docs, doc_scores, doc_ids_for_topic = model.search_documents_by_topic(topic_num=tnum, num_docs=num_to_get)\n",
    "        except Exception:\n",
    "            # if that fails, skip this topic\n",
    "            continue\n",
    "        for did in doc_ids_for_topic:\n",
    "            docid_to_topic[str(did)] = int(tnum)\n",
    "\n",
    "    # Map dominant topic\n",
    "    df[\"top2vec_dominant_topic\"] = df[doc_id_col].astype(str).map(docid_to_topic).fillna(-1).astype(int)\n",
    "\n",
    "    # Top N\n",
    "    df[\"top2vec_top_topics\"] = df[\"top2vec_dominant_topic\"].apply(lambda x: [int(x)] + [-1] * (topN - 1))\n",
    "    for i in range(topN):\n",
    "        df[f\"top2vec_topic_rank_{i+1}\"] = df[\"top2vec_top_topics\"].apply(lambda l: int(l[i]) if i < len(l) else -1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_topic_label_map(model: Top2Vec, top_k_words: int = 5) -> Dict[int, str]:\n",
    "    n_topics = model.get_num_topics()\n",
    "    topic_words, word_scores, topic_nums = model.get_topics(n_topics)\n",
    "    label_map = {}\n",
    "    for tnum, words in zip(topic_nums, topic_words):\n",
    "        words_sel = words[:top_k_words]\n",
    "        label_map[int(tnum)] = \", \".join(words_sel)\n",
    "    return label_map\n",
    "\n",
    "\n",
    "def build_topic_summary_df(model: Top2Vec, df: pd.DataFrame, text_col: str, top_words: int = 10, sample_docs: int = 5) -> pd.DataFrame:\n",
    "    topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "    # to gather words: request all topics\n",
    "    n_topics = len(topic_nums)\n",
    "    topic_words_all, _, topic_numbers_all = model.get_topics(n_topics)\n",
    "\n",
    "    rows = []\n",
    "    for i, tnum in enumerate(topic_nums):\n",
    "        size = int(topic_sizes[i])\n",
    "        # find words corresponding to tnum (topic_numbers_all may be same order as topic_nums)\n",
    "        idx = list(topic_numbers_all).index(tnum) if tnum in topic_numbers_all else i\n",
    "        words = topic_words_all[idx][:top_words]\n",
    "        # sample docs\n",
    "        num_to_get = min(sample_docs, size if size > 0 else 1)\n",
    "        docs, doc_scores, doc_ids = model.search_documents_by_topic(topic_num=tnum, num_docs=num_to_get)\n",
    "        sample_texts = []\n",
    "        for did, doc_text in zip(doc_ids, docs):\n",
    "            # try to map back to original df text if present\n",
    "            mask = df[\"comment_id\"].astype(str) == str(did)\n",
    "            if mask.any():\n",
    "                sample_texts.append(df.loc[mask, text_col].iloc[0])\n",
    "            else:\n",
    "                sample_texts.append(doc_text)\n",
    "        rows.append({\n",
    "            \"topic_num\": int(tnum),\n",
    "            \"size\": size,\n",
    "            \"top_words\": \", \".join(words),\n",
    "            \"sample_comments\": \" ||| \".join([s[:400].replace(\"\\n\", \" \") for s in sample_texts])\n",
    "        })\n",
    "    summary_df = pd.DataFrame(rows).sort_values(\"size\", ascending=False).reset_index(drop=True)\n",
    "    return summary_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3472e25a-d792-4779-9a51-8fd4b2ccf332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 16:59:30,069 - top2vec - INFO - Pre-processing documents for training\n",
      "2025-09-05 16:59:30,155 - top2vec - INFO - Creating joint document/word embedding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12437 comments from C:\\Users\\linna\\OneDrive\\Documents\\Python_Dev\\topic-modeling\\data\\public_comments.json\n",
      "Filtered to docket 'TTB-2025-0003': 189 comments\n",
      "Docs in df: 189\n",
      "Training Top2Vec:\n",
      "  embedding_model=doc2vec, speed=deep-learn, workers=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 16:59:43,458 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2025-09-05 16:59:56,079 - top2vec - INFO - Finding dense areas of documents\n",
      "C:\\Users\\linna\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\linna\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "2025-09-05 16:59:56,095 - top2vec - INFO - Finding topics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "Number of topics discovered: 2\n",
      "Top 10 topic sizes (docs per topic):\n",
      "  Topic 0: 118 documents\n",
      "  Topic 1: 71 documents\n",
      "Topic summary saved to C:\\Users\\linna\\OneDrive\\Documents\\Python_Dev\\topic-modeling\\outputs\\top2vec_topic_summary.csv\n",
      "Comments with topic columns saved to C:\\Users\\linna\\OneDrive\\Documents\\Python_Dev\\topic-modeling\\outputs\\comments_with_top2vec.csv\n",
      "Model saved to: C:\\Users\\linna\\Documents\\top2vec_model_classic.pkl\n",
      "topic_summary (top rows):\n",
      " topic_num  size                                                                       top_words                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      sample_comments\n",
      "         0   118                        be, an, these, there, this, their, is, label, from, that                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          This policy should have positive effects as it keeps customers informed about what they are consuming. This could have economic effects on smaller businesses who have to no provide additional labeling.  ||| Please require ingredients and all additives or adjuncts to protect people from allergens ||| I am in support of this, but the third exception (TTB Petition) that is stated here seems really vague, and I think it could use some quantitative data behind it to help people who are considering using this option have a bit more clarity. ||| I am very disappointed with this agency. I do not want the beautiful bottles in the liquor store to be flooded with regulatory comments and warnings about the consequences of drinking/ the hazards that could come with boozing on a Wednesday night. If people are worried about their health concerns or allergies, then they probably should not be drinking. I think I speak for all working class America ||| Allergen information and nutritional labels should be provided on every package so that consumers are best able to make fully educated decisions about the products they are consuming. There should be no exceptions\n",
      "         1    71 consumers, information, with, ttb, or, rule, proposed, consumer, public, safety I am writing in support of the proposed rule requiring allergen labeling on wines, distilled spirits, and malt beverages. I believe consumers deserve to know what&rsquo;s in the products they consume, especially those with life-threatening food allergies. This rule helps protect public health through greater transparency. While this may mean increased costs for producers, I believe the benefits to ||| Hello, my name is Hunter Carlson, and I am a first-year university student in Virginia. I support the proposed allergen labeling rule. Clear ingredient information on alcohol products is essential for public health and consumer safety, especially for those with severe allergies. Requiring this labeling would bring much-needed transparency to the industry and help protect vulnerable consumers.<br/> ||| TTB&#39;s proposed allergen labeling rule is critical for consumer safety, aligning alcohol with FALCPA standards. Mandating clear disclosures of major allergens prevents life-threatening reactions. The 5-year compliance period balances industry adaptation. Ensure exceptions (e.g., allergen removal processes) are science-based and transparent. Support timely implementation to protect public health ||| Dear Alcohol and Tobacco Tax and Trade Bureu Officials,<br/><br/>I am writing in support of the Alcohol and Tobacco Tax and Trade Bureau&rsquo;s proposal to require labeling disclosures for major food allergens used in the production of alcohol beverages.<br/><br/>This is an important and necessary step toward greater transparency and consumer protection. Many individuals, like myself, suffer from ||| Public Comment on Notice No. 238 &ndash; Major Food Allergen Labeling for Alcohol Beverages<br/><br/>I am writing in support of TTB&rsquo;s proposed rule to require mandatory labeling of major food allergens on alcohol beverages. As someone who works at Walgreens, where we operate a liquor store, I have seen firsthand how alcohol products currently lack clear allergen labeling. While I have not ye\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # load and filter\n",
    "    df = load_data(DATA_PATH)\n",
    "    df = filter_by_docket(df, DOCKET_TO_USE)\n",
    "    df = df.dropna(subset=[TEXT_COL]).reset_index(drop=True)\n",
    "    print(\"Docs in df:\", len(df))\n",
    "\n",
    "    # prepare docs and ids\n",
    "    documents = df[TEXT_COL].astype(str).tolist()\n",
    "    document_ids = df[DOC_ID_COL].astype(str).tolist()\n",
    "\n",
    "    # train\n",
    "    model = train_top2vec(\n",
    "        documents=documents,\n",
    "        document_ids=document_ids,\n",
    "        embedding_model=EMBEDDING_MODEL,\n",
    "        speed=SPEED,\n",
    "        workers=WORKERS\n",
    "    )\n",
    "\n",
    "    # outputs\n",
    "    n_topics = model.get_num_topics()\n",
    "    print(\"Number of topics discovered:\", n_topics)\n",
    "    topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "    print(\"Top 10 topic sizes (docs per topic):\")\n",
    "    for size, num in zip(topic_sizes[:10], topic_nums[:10]):\n",
    "        print(f\"  Topic {num}: {size} documents\")\n",
    "\n",
    "    # map topics back to df (dominant + topN)\n",
    "    df = map_dominant_and_topN(model, df, DOC_ID_COL, TEXT_COL, topN=3)\n",
    "\n",
    "    # label topics using top terms\n",
    "    topic_label_map = build_topic_label_map(model, top_k_words=TOP_N_FOR_LABEL)\n",
    "    df[\"top2vec_terms\"] = df[\"top2vec_dominant_topic\"].map(topic_label_map).fillna(\"Unclear\")\n",
    "\n",
    "    # topic summary table\n",
    "    topic_summary = build_topic_summary_df(model, df, TEXT_COL, top_words=TOP_WORDS_PER_TOPIC, sample_docs=SAMPLE_DOCS_PER_TOPIC)\n",
    "    topic_summary.to_csv(TOPIC_SUMMARY_CSV, index=False)\n",
    "    print(f\"Topic summary saved to {TOPIC_SUMMARY_CSV}\")\n",
    "\n",
    "    # save df with topic columns\n",
    "    df.to_csv(OUTPUT_DF_CSV, index=False)\n",
    "    print(f\"Comments with topic columns saved to {OUTPUT_DF_CSV}\")\n",
    "\n",
    "    # save model\n",
    "    model.save(str(MODEL_SAVE_FILE))\n",
    "    print(\"Model saved to:\", MODEL_SAVE_FILE)\n",
    "    # temporary until repo is finalized \n",
    "    # MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    # model.save(str(MODEL_SAVE_DIR))\n",
    "    # print(\"Model saved to:\", MODEL_SAVE_DIR)\n",
    "\n",
    "    # check\n",
    "    print(\"topic_summary (top rows):\")\n",
    "    print(topic_summary.head(10).to_string(index=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
