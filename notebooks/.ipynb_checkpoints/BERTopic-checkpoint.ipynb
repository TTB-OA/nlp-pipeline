{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1707e041-8c68-43f2-9190-a8c83c7affae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f50b12-fdf5-43a4-a57a-1aa084c5b2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# parameters #\n",
    "##############\n",
    "\n",
    "DATA_PATH = Path(r\"C:\\Users\\linna\\OneDrive\\Documents\\Python_Dev\\topic-modeling\\data\\public_comments.json\")\n",
    "\n",
    "TEXT_COL = \"comment_text\"\n",
    "DOC_ID_COL = \"comment_id\"\n",
    "DOCKET_TO_USE = \"TTB-2025-0003\"\n",
    "\n",
    "# repo / outputs\n",
    "try:\n",
    "    REPO_ROOT = Path(__file__).parent.parent.resolve()\n",
    "except NameError:\n",
    "    REPO_ROOT = Path(os.getcwd()).parent.resolve()\n",
    "\n",
    "OUTPUTS_DIR = REPO_ROOT / \"outputs\"\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TOPIC_SUMMARY_CSV = OUTPUTS_DIR / \"bertopic_topic_summary.csv\"\n",
    "OUTPUT_DF_CSV = OUTPUTS_DIR / \"comments_with_bertopic.csv\"\n",
    "MODEL_SAVE_FILE = Path(r\"C:\\Users\\linna\\Documents\\bertopic_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af8b862f-9b96-4b42-b35a-c79c752771cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding + clustering / reduction params\n",
    "EMBEDDING_MODEL = \"all-mpnet-base-v2\"   # richer embedding (change if you prefer)\n",
    "CHUNK_DOCS = False                      # set True to chunk docs into smaller pieces (experimental)\n",
    "\n",
    "# UMAP (dim reduction) controls (affects granularity of embeddings before clustering)\n",
    "UMAP_N_NEIGHBORS = 15\n",
    "UMAP_MIN_DIST = 0.1\n",
    "UMAP_N_COMPONENTS = 5\n",
    "\n",
    "# HDBSCAN controls (controls cluster granularity)\n",
    "HDBSCAN_MIN_CLUSTER_SIZE = 2    # smaller => more, smaller topics (tune this)\n",
    "HDBSCAN_MIN_SAMPLES = 1\n",
    "\n",
    "# output params\n",
    "TOP_WORDS_PER_TOPIC = 10\n",
    "SAMPLE_DOCS_PER_TOPIC = 5\n",
    "TOP_N_FOR_LABEL = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36463ac5-75b0-41eb-8216-d68174e52180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_json(path, orient=\"records\", lines=False)\n",
    "\n",
    "    if TEXT_COL not in df.columns:\n",
    "        raise ValueError(f\"{TEXT_COL} not found in dataframe columns: {df.columns.tolist()}\")\n",
    "\n",
    "    if \"comment_title\" in df.columns:\n",
    "        # deduplicate mass comments\n",
    "        pattern = re.compile(r'^\\s*Mass Comment\\s*[#\\(\\-:\\s]*\\s*(\\d+)', flags=re.IGNORECASE)\n",
    "\n",
    "        def _extract_mass_num(title):\n",
    "            if not isinstance(title, str):\n",
    "                return None\n",
    "            m = pattern.match(title)\n",
    "            if m:\n",
    "                try:\n",
    "                    return int(m.group(1))\n",
    "                except ValueError:\n",
    "                    return None\n",
    "            return None\n",
    "\n",
    "        df[\"__mass_num\"] = df[\"comment_title\"].apply(_extract_mass_num)\n",
    "\n",
    "        # keep first occurrence for each mass_num, drop subsequent ones\n",
    "        mask_mass = df[\"__mass_num\"].notna()\n",
    "        before_len = len(df)\n",
    "        duplicated_mask = df.loc[mask_mass, \"__mass_num\"].duplicated(keep=\"first\")\n",
    "        dup_index = df.loc[mask_mass].index[duplicated_mask]\n",
    "        if len(dup_index) > 0:\n",
    "            df = df.drop(index=dup_index).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.reset_index(drop=True)\n",
    "        after_len = len(df)\n",
    "        print(f\"Dropped {before_len - after_len} duplicate 'Mass Comment N' rows (kept first of each N).\")\n",
    "\n",
    "        df = df.drop(columns=\"__mass_num\")\n",
    "        \n",
    "    df = df.dropna(subset=[TEXT_COL]).reset_index(drop=True)\n",
    "    print(f\"Loaded {len(df)} comments from {path}\")\n",
    "    return df\n",
    "\n",
    "def filter_by_docket(df: pd.DataFrame, docket: str | None) -> pd.DataFrame:\n",
    "    if docket is None:\n",
    "        return df\n",
    "    if \"docket_id\" not in df.columns:\n",
    "        raise ValueError(\"docket_id column not in dataframe\")\n",
    "    df_sub = df[df[\"docket_id\"] == docket].reset_index(drop=True)\n",
    "    print(f\"Filtered to docket '{docket}': {len(df_sub)} comments\")\n",
    "    return df_sub\n",
    "\n",
    "def chunk_documents(documents: List[str], chunk_size: int = 200, overlap: int = 50) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Simple sliding-window chunker returning (chunks, chunk_origin_doc_id)\n",
    "    chunk_size = number of tokens (approx via split) per chunk\n",
    "    overlap = overlapping tokens between chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    origin_ids = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        toks = str(doc).split()\n",
    "        if len(toks) <= chunk_size:\n",
    "            chunks.append(\" \".join(toks))\n",
    "            origin_ids.append(i)\n",
    "        else:\n",
    "            start = 0\n",
    "            while start < len(toks):\n",
    "                end = min(len(toks), start + chunk_size)\n",
    "                chunks.append(\" \".join(toks[start:end]))\n",
    "                origin_ids.append(i)\n",
    "                if end == len(toks):\n",
    "                    break\n",
    "                start = end - overlap\n",
    "    return chunks, origin_ids\n",
    "\n",
    "def train_bertopic(documents: List[str], embedding_model_name: str, verbose: bool = True) -> Tuple[BERTopic, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Train BERTopic with a SentenceTransformer embedding model and custom UMAP/HDBSCAN.\n",
    "    Returns (model, embeddings)\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Loading embedding model:\", embedding_model_name)\n",
    "    embedder = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "    # compute embeddings first (we can reuse them)\n",
    "    if verbose:\n",
    "        print(\"Computing embeddings for\", len(documents), \"documents...\")\n",
    "    embeddings = embedder.encode(documents, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "    # build UMAP and HDBSCAN instances with tunable params\n",
    "    umap_model = umap.UMAP(n_neighbors=UMAP_N_NEIGHBORS, n_components=UMAP_N_COMPONENTS, min_dist=UMAP_MIN_DIST, metric=\"cosine\", random_state=42)\n",
    "    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=HDBSCAN_MIN_CLUSTER_SIZE, min_samples=HDBSCAN_MIN_SAMPLES, metric=\"euclidean\", cluster_selection_method=\"eom\", prediction_data=True)\n",
    "\n",
    "    # instantiate BERTopic with our reducers/clusters\n",
    "    topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model, calculate_probabilities=True, verbose=verbose)\n",
    "    if verbose:\n",
    "        print(\"Training BERTopic...\")\n",
    "    topics, probs = topic_model.fit_transform(documents, embeddings)\n",
    "    if verbose:\n",
    "        print(\"BERTopic training complete. Generated\", len(set(topics)) - (1 if -1 in topics else 0), \"non-outlier topics (excludes -1).\")\n",
    "    return topic_model, embeddings\n",
    "\n",
    "def map_dominant_and_topN_bertopic(model: BERTopic, documents: List[str], df: pd.DataFrame,\n",
    "                                   doc_id_col: str, embeddings: np.ndarray | None = None,\n",
    "                                   topN: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds:\n",
    "      - bertopic_dominant_topic\n",
    "      - bertopic_top_topics (list)\n",
    "      - bertopic_topic_rank_{i}\n",
    "    Pass 'embeddings' (precomputed) to avoid BERTopic trying to re-embed.\n",
    "    \"\"\"\n",
    "    # If we have embeddings, pass them; otherwise let BERTopic try (may fail if embedding_model is None)\n",
    "    if embeddings is not None:\n",
    "        topics, probs = model.transform(documents, embeddings=embeddings)\n",
    "    else:\n",
    "        topics, probs = model.transform(documents)  # may raise if model.embedding_model is None\n",
    "\n",
    "    df[\"bertopic_dominant_topic\"] = topics\n",
    "\n",
    "    # compute topN from probs if available\n",
    "    if probs is not None:\n",
    "        probs_arr = np.array(probs)\n",
    "        if probs_arr.ndim == 2:\n",
    "            top_indices = np.argsort(probs_arr, axis=1)[:, ::-1][:, :topN]\n",
    "            top_lists = top_indices.tolist()\n",
    "            topic_info = model.get_topic_info().reset_index(drop=True)\n",
    "            topic_ids_order = topic_info[\"Topic\"].tolist()\n",
    "            idx_to_topic = {i: tid for i, tid in enumerate(topic_ids_order)}\n",
    "            top_topics = [[idx_to_topic.get(i, -1) for i in lst] for lst in top_lists]\n",
    "            df[\"bertopic_top_topics\"] = top_topics\n",
    "            for i in range(topN):\n",
    "                df[f\"bertopic_topic_rank_{i+1}\"] = df[\"bertopic_top_topics\"].apply(lambda l: l[i] if i < len(l) else -1)\n",
    "        else:\n",
    "            df[\"bertopic_top_topics\"] = df[\"bertopic_dominant_topic\"].apply(lambda x: [int(x)] + [-1]*(topN-1))\n",
    "            for i in range(topN):\n",
    "                df[f\"bertopic_topic_rank_{i+1}\"] = df[\"bertopic_top_topics\"].apply(lambda l: l[i] if i < len(l) else -1)\n",
    "    else:\n",
    "        df[\"bertopic_top_topics\"] = df[\"bertopic_dominant_topic\"].apply(lambda x: [int(x)] + [-1]*(topN-1))\n",
    "        for i in range(topN):\n",
    "            df[f\"bertopic_topic_rank_{i+1}\"] = df[\"bertopic_top_topics\"].apply(lambda l: l[i] if i < len(l) else -1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_topic_summary_bertopic(model: BERTopic, df: pd.DataFrame, documents: List[str],\n",
    "                                 doc_id_col: str, top_words: int = 10, sample_docs: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a topic summary DataFrame similar to your Top2Vec output:\n",
    "      columns: topic_num, size, top_words (comma sep), sample_comments (|||)\n",
    "    This version avoids variable shadowing and safely handles different\n",
    "    return types from model.get_representative_docs(...)\n",
    "    \"\"\"\n",
    "    info = model.get_topic_info()  # DataFrame with 'Topic' and 'Count' and 'Name'\n",
    "    rows = []\n",
    "    for _, row in info.iterrows():\n",
    "        tnum = int(row[\"Topic\"])\n",
    "        size = int(row[\"Count\"])\n",
    "\n",
    "        # Get top words for topic (BERTopic returns list of (word, score))\n",
    "        topic_words = model.get_topic(tnum)\n",
    "        if topic_words:\n",
    "            words = [w for w, s in topic_words][:top_words]\n",
    "        else:\n",
    "            words = []\n",
    "\n",
    "        # --- get representative docs safely ---\n",
    "        rep_docs_list = []\n",
    "        try:\n",
    "            rep = model.get_representative_docs(tnum)\n",
    "            # rep may be None, list, tuple, np.ndarray, or other iterable\n",
    "            if rep is None:\n",
    "                rep_docs_list = []\n",
    "            elif isinstance(rep, (list, tuple)):\n",
    "                rep_docs_list = list(rep)[:sample_docs]\n",
    "            else:\n",
    "                # try to coerce to list (covers numpy arrays, pandas Series, generators)\n",
    "                try:\n",
    "                    rep_docs_list = list(rep)[:sample_docs]\n",
    "                except Exception:\n",
    "                    rep_docs_list = []\n",
    "        except Exception:\n",
    "            # If get_representative_docs throws, fallback to dataframe selection below\n",
    "            rep_docs_list = []\n",
    "\n",
    "        # If no representative docs from model, fallback to df rows for that topic\n",
    "        if not rep_docs_list:\n",
    "            try:\n",
    "                mask = df[\"bertopic_dominant_topic\"] == tnum\n",
    "                rep_docs_list = df.loc[mask, TEXT_COL].astype(str).tolist()[:sample_docs]\n",
    "            except Exception:\n",
    "                rep_docs_list = []\n",
    "\n",
    "        # Prepare sample_texts (truncate and clean)\n",
    "        sample_texts = []\n",
    "        for s in rep_docs_list:\n",
    "            try:\n",
    "                s_str = str(s).replace(\"\\n\", \" \")\n",
    "            except Exception:\n",
    "                s_str = \"\"\n",
    "            sample_texts.append(s_str[:400])\n",
    "\n",
    "        rows.append({\n",
    "            \"topic_num\": tnum,\n",
    "            \"size\": size,\n",
    "            \"top_words\": \", \".join(words),\n",
    "            \"sample_comments\": \" ||| \".join(sample_texts)\n",
    "        })\n",
    "\n",
    "    summary_df = pd.DataFrame(rows).sort_values(\"size\", ascending=False).reset_index(drop=True)\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "259105d9-97f4-4da1-8f2f-bdd665aa005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ main flow ------------------ #\n",
    "def main_bertopic():\n",
    "    df = load_data(DATA_PATH)\n",
    "    df = filter_by_docket(df, DOCKET_TO_USE)\n",
    "    df = df.dropna(subset=[TEXT_COL]).reset_index(drop=True)\n",
    "    print(\"Docs in df:\", len(df))\n",
    "\n",
    "    # optionally chunk documents to increase micro-topic detection\n",
    "    if CHUNK_DOCS:\n",
    "        docs = df[TEXT_COL].astype(str).tolist()\n",
    "        chunks, origin_ids = chunk_documents(docs, chunk_size=120, overlap=30)\n",
    "        print(\"Created\", len(chunks), \"chunks from\", len(docs), \"original docs.\")\n",
    "        train_docs = chunks\n",
    "        # keep mapping back to original doc index for summarization if needed\n",
    "        chunk_origin = origin_ids\n",
    "    else:\n",
    "        train_docs = df[TEXT_COL].astype(str).tolist()\n",
    "        chunk_origin = None\n",
    "\n",
    "    # Train BERTopic\n",
    "    model, embeddings = train_bertopic(train_docs, EMBEDDING_MODEL)\n",
    "    # ...\n",
    "    df = map_dominant_and_topN_bertopic(model, train_docs, df, DOC_ID_COL, embeddings=embeddings, topN=3)\n",
    "\n",
    "    # label topics and build summary\n",
    "    topic_summary = build_topic_summary_bertopic(model, df, train_docs, DOC_ID_COL, top_words=TOP_WORDS_PER_TOPIC, sample_docs=SAMPLE_DOCS_PER_TOPIC)\n",
    "    topic_summary.to_csv(TOPIC_SUMMARY_CSV, index=False)\n",
    "    print(\"Topic summary saved to:\", TOPIC_SUMMARY_CSV)\n",
    "\n",
    "    # save df with topics\n",
    "    df.to_csv(OUTPUT_DF_CSV, index=False)\n",
    "    print(\"Comments with topic columns saved to:\", OUTPUT_DF_CSV)\n",
    "\n",
    "    # save model\n",
    "    model.save(str(MODEL_SAVE_FILE))\n",
    "    print(\"Model saved to:\", MODEL_SAVE_FILE)\n",
    "\n",
    "    print(\"Top rows of topic_summary:\")\n",
    "    print(topic_summary.head(10).to_string(index=False))\n",
    "\n",
    "    return model, df, topic_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5a72ab6-86de-4c77-8164-4fb16414bf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12437 comments from C:\\Users\\linna\\OneDrive\\Documents\\Python_Dev\\topic-modeling\\data\\public_comments.json\n",
      "Filtered to docket 'TTB-2025-0003': 189 comments\n",
      "Docs in df: 189\n",
      "Loading embedding model: all-mpnet-base-v2\n",
      "Computing embeddings for 189 documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569e21736c564776ac73b4005e7975e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 10:42:21,370 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERTopic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 10:42:32,962 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-08 10:42:32,962 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-08 10:42:33,059 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-08 10:42:33,068 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-09-08 10:42:33,209 - BERTopic - Representation - Completed ✓\n",
      "2025-09-08 10:42:33,454 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-09-08 10:42:33,461 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-08 10:42:33,463 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-09-08 10:42:33,478 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
      "2025-09-08 10:42:33,552 - BERTopic - Probabilities - Completed ✓\n",
      "2025-09-08 10:42:33,553 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-08 10:42:33,644 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTopic training complete. Generated 49 non-outlier topics (excludes -1).\n",
      "Topic summary saved to: C:\\Users\\linna\\OneDrive\\Documents\\Python_Dev\\topic-modeling\\outputs\\bertopic_topic_summary.csv\n",
      "Comments with topic columns saved to: C:\\Users\\linna\\OneDrive\\Documents\\Python_Dev\\topic-modeling\\outputs\\comments_with_bertopic.csv\n",
      "Model saved to: C:\\Users\\linna\\Documents\\bertopic_model\n",
      "Top rows of topic_summary:\n",
      " topic_num  size                                                                             top_words                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            sample_comments\n",
      "        -1    17                                       of, and, the, food, for, is, from, in, this, to To whom it may concern,<br/><br/>We are a group of allergen-concerned citizens who wish to convey our support for the proposed regulation. Thank you for your time and consideration of our stance on this issue. Approximately 33 million Americans have major food allergies which affect their everyday life (Food Allergy Research &amp; Education Organization, 2024). It is imperative that they are able  ||| Overall, following FDA&rsquo;s current requirements for allergen labeling on food products would be most beneficial for consumers. Consumers are already accustomed to seeing &ldquo;Contains:___&rdquo; on food labels. TTB&rsquo;s proposed wording, &ldquo;Contains Major Food Allergen:____&rdquo; may be confusing to consumers. While food industry professionals know that &ldquo;major food allergens&rd ||| My name is Annika Dhawan and I am a third year MD/MPH students at a university-affiliated hospital in South Florida.<br/>The regulation I wish to provide commentary on is RIN 1513-AC94, Major Food Allergen Labelling for Wines, Distilled Spirits, and Malt Beverages proposed by the Alcohol and Tobacco Tax and Trade Bureau (TTB) on 01/17/2025. This regulation proposes to require disclosure of all maj\n",
      "         0     9                              files, attached, see, document, full, comment, for, , ,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   See attached file(s) ||| See attached file(s) ||| See attached document for full comment \n",
      "         1     7 ingredients, nature, ingredient, these, hidden, rely, please, dairy, given, allergens As a Registered Dietitian working closely with patients who have allergies, I strongly support allergen labeling on alcohol products. Many individuals with food allergies rely on clear ingredient information to avoid potential health risks. Since alcohol can contain hidden allergens such as gluten, nuts, sulfites, or artificial additives, transparent labeling is essential for their safety.<br/><br ||| It&rsquo;s shocking to me that ingredients and major allergens are not already required on these consumable items. It seems the alcohol industry would&rsquo;ve wanted to reduce their liability by including all ingredients and labeling for major allergens, especially given the litigious nature of our society. Liabilities aside, common sense labeling is just common sense. I am respectfully requestin ||| Please include major food allergens on alcoholic beverage labels. These drinks should be viewed no differently than other beverages and foods which require the notice of these ingredients because they could potentially harm the safety and well-being of the consumers. Alcoholic beverages are no different. To assure the greatest safety for the consumers, these ingredients should be bolded and separa\n",
      "         3     6             the, nutritional, serving, calories, per, public, health, to, calorie, of The proposed rule by the TTB is a great rule especially with how in the past allergens where listed as an option. Allergens like milk, fish, soy and peanuts are important due to how an allergic reaction can happen or could potentially kill the drinker,  By the Brewers Association the rule it will also disclose, Serving size, number of servings per container, alcohol content, number of ounces per s ||| I believe that the proposed rule will be of much benefit to the American consumer, primarily to the reinforcement of informed consent in the purchasing of products. The fact it has taken so long to require allergen labels onto alcoholic beverages is quite alarming, especially while the market for alcoholic beverages and breweries has exploded. According to the Brewers Association, the number of br ||| The proposed rule by the TTB is a great rule especially with how in the past allergens were listed as an option. Allergens like milk, fish, soy and peanuts are important due to how an allergic reaction can happen or could potentially kill the drinker,  By the Brewers Association the rule it will also disclose, Serving size, number of servings per container, alcohol content, number of ounces per se\n",
      "         4     6            and, to, rule, for, information, not, with, proposed, consumers, allergies Dear Alcohol and Tobacco Tax and Trade Bureau, <br/><br/>As a consumer, I feel alcoholic beverages should be required to disclose ingredients and major food allergens. Please act swiftly to finalize the proposed rule: Major Food Allergen Labeling for Wines, Distilled Spirits, and Malt Beverages, and require on-package labeling of major food allergens. <br/><br/>Roughly 11% of U.S. adults have food ||| To Whom It May Concern,<br/>Thank you for the opportunity to comment on this important rulemaking to require allergen labeling for alcoholic beverages. Recover Alaska strongly supports this proposed rule as a meaningful step forward in protecting public health and advancing transparency for all consumers.<br/>Recover Alaska is a statewide coalition dedicated to reducing excessive alcohol use and i ||| As a parent and public health professional, I am writing in strong support of the proposed rule to require allergen labeling on alcoholic beverages. I know firsthand how critical it is to have clear, accessible information about what&rsquo;s in our food and drinks&mdash;for safety, for health, and for peace of mind.<br/><br/>More than 1 in 10 U.S. adults have food allergies, and the consequences o\n",
      "         2     6             ingredient, alcohol, mdash, public, that, health, full, the, of, consumer Subject: Strong Support for Allergen Labeling &mdash; and Urgent Call for Full Ingredient Disclosure<br/><br/>To Whom It May Concern:<br/><br/>I&rsquo;m writing in strong support of the proposed rule TTB‑2025‑0003 requiring mandatory allergen labeling on all alcoholic beverages. This is a critical step toward protecting consumer health. However, I believe this proposal should go further &mdash; we ||| On behalf of A Voice for Choice Advocacy, we believe that the Alcohol and Tobacco Tax and Trade Bureau (TTB) should mandate ingredient labeling of all contents used in the production of alcoholic beverages regulated under the Federal Alcohol Administration Act. This requirement is essential for several reasons.<br/> <br/>Firstly, food allergies are a significant public health concern. According to ||| On behalf of the Project Extra Mile coalition of more than 400 Nebraskans working to prevent alcohol-related harms resulting from excessive alcohol consumption, we urge the Alcohol and Tobacco Tax Trade Bureau to implement guidelines requiring the disclosure of nutrition, allergen, and ingredient information on labeling serves an important public health purpose. Increased transparency is warranted\n",
      "         9     5        wineries, small, fining, wine, scientific, agents, studies, in, testing, ndash De Maison Selections Inc. is a small business with 20 employees that imports fine wines, ciders, and spirits from small, family-owned wineries in France and Spain. We are addressing the questions the TTB raised in the notice of proposed rulemaking.   <br/><br/>Overall Comment on Proposed Rulemaking:<br/>Complying with the proposed regulation would result in up-front costs in both time and money. W ||| The proposed rule could significantly impact small wineries, particularly in terms of compliance costs, operational adjustments, and potential liability. Here&rsquo;s why:<br/><br/>Impact on Small Wineries:<br/><br/>Ingredient Disclosure Complexity &ndash; While wineries don&rsquo;t typically use major allergens as ingredients, some allergens&mdash;like egg whites (for fining), milk-based casein ( ||| Dear Administrator,<br/><br/>On behalf of the Georgia Wine Producers, representing 110 member wineries across Georgia, we appreciate the opportunity to submit comments regarding TTB Notice No. 238. We respect TTB&rsquo;s mission to ensure consumer safety and transparency, but must respectfully oppose the proposed mandatory allergen labeling regulations for alcoholic beverages.<br/><br/>Our positio\n",
      "        14     5    what, retailers, suppliers, consuming, effects, this, will, warning, recommend, it                                                                                                                                                 This rule would keep our community safer by protecting people with allergens. People deserve to know exactly what is in what they&rsquo;re consuming. If there is any amount of the substance in the beverage, it should be reported. While I understand that this information makes packaging less physically appealing, the overall benefit to the community&rsquo;s safety is worth it. <br/> ||| I would recommend considering how this affects small producers who might not have the scale to produce these labels for their products. While the purpose of this regulation would be beneficial to the public, I suggest that other methods of warning labels also be introduced and accepted, like the use of QR codes and warning notices at official places of sale instead of on each bottle or can. ||| What will you do about 50ml? The print will be too small for it to be read. If it is a requirement on all sizes This will harm trial of products (causing issues for both consumers, retailers and suppliers) and will also reduce profits for retailers and suppliers on many levels. \n",
      "        11     5 labeling, aligns, beverages, as, ttbrsquos, alcohol, consumer, window, food, allergic To whom it may concern and those making decisions;<br/><br/>I am strongly in favor of at least some ingredient listing requirements for the beverages under TTB&#39;s ruling. At the grocery store I check ingredients (when I can actually read them!), but more importantly I look for the &quot;Contains:...&quot; at the bottom of the ingredients list and I look to see if there&#39;s any mention of gene ||| Public Comment on Notice No. 238 &ndash; Major Food Allergen Labeling for Alcohol Beverages<br/><br/>I am writing in support of TTB&rsquo;s proposed rule to require mandatory labeling of major food allergens on alcohol beverages. As someone who works at Walgreens, where we operate a liquor store, I have seen firsthand how alcohol products currently lack clear allergen labeling. While I have not ye ||| To Whom It May Concern:<br/><br/>I write in strong support of the Alcohol and Tobacco Tax and Trade Bureau&rsquo;s (TTB) proposed rule to require the labeling of major food allergens on alcoholic beverages subject to TTB&rsquo;s regulatory authority.<br/><br/>The inclusion of clear and consistent allergen labeling is long overdue. Consumers with food allergies must be able to make informed decisio\n",
      "        10     5             major, alcohol, used, allergens, beverages, trade, tax, food, in, tobacco Dear Alcohol and Tobacco Tax and Trade Bureau,<br/><br/>I am writing to you in support of this proposed rule. As someone with food allergies that drinks &mdash; and is loath to have to look up what is in the alcoholic libation they are considering purchasing &mdash; requiring the listing of the major food allergens that are found in the alcoholic beverage would be a very good &quot;quality of life ||| The Alcohol and Tobacco Tax and Trade Bureau (TTB) should require a labeling disclosure of all major food allergens used in the production of alcohol beverages subject to TTB&#39;s regulatory authority under the Federal Alcohol Administration Act with no exceptions. Not disclosing all major food allergens used in the production of alcohol beverages is dangerous and exceeds misinformation. Competit ||| The Alcohol and Tobacco Tax and Trade Bureau (TTB) should require a labeling disclosure of all major food allergens used in the production of alcohol beverages subject to TTB&#39;s regulatory authority under the Federal Alcohol Administration Act with no exceptions. <br/><br/>Not disclosing all major food allergens used in the production of alcohol beverages is dangerous and exceeds misinformation\n"
     ]
    }
   ],
   "source": [
    "model, df_out, topic_summary = main_bertopic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0cb0ed-e003-46dd-aada-153268f3dffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2883497-c781-4ed0-8bad-aff58b8a4525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
