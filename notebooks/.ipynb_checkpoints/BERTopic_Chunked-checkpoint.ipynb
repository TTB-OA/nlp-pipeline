{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1707e041-8c68-43f2-9190-a8c83c7affae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# imports #\n",
    "###########\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f50b12-fdf5-43a4-a57a-1aa084c5b2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# parameters #\n",
    "##############\n",
    "\n",
    "DATA_PATH = Path(r\"C:\\Users\\linna\\OneDrive\\Documents\\Python_Dev\\topic-modeling\\data\\comments_09DEC2025.json\")\n",
    "\n",
    "TEXT_COL = \"comment_text\"\n",
    "DOC_ID_COL = \"comment_id\"\n",
    "DOCKET_TO_USE = \"TTB-2025-0002\"\n",
    "\n",
    "# repo / outputs\n",
    "try:\n",
    "    REPO_ROOT = Path(__file__).parent.parent.resolve()\n",
    "except NameError:\n",
    "    REPO_ROOT = Path(os.getcwd()).parent.resolve()\n",
    "\n",
    "OUTPUTS_DIR = REPO_ROOT / \"outputs\"\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TOPIC_SUMMARY_CSV = OUTPUTS_DIR / \"bertopic_topic_summary.csv\"\n",
    "OUTPUT_DF_CSV = OUTPUTS_DIR / \"comments_with_bertopic.csv\"\n",
    "MODEL_SAVE_FILE = Path(r\"C:\\Users\\linna\\Documents\\bertopic_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36463ac5-75b0-41eb-8216-d68174e52180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# functions #\n",
    "#############\n",
    "\n",
    "def load_data(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_json(path, orient=\"records\", lines=False)\n",
    "\n",
    "    if TEXT_COL not in df.columns:\n",
    "        raise ValueError(f\"{TEXT_COL} not found in dataframe columns: {df.columns.tolist()}\")\n",
    "\n",
    "    if \"comment_title\" in df.columns:\n",
    "        # deduplicate mass comments, compute docket-level counts\n",
    "        pattern = re.compile(r'^\\s*Mass Comment\\s*[#\\(\\-:\\s]*\\s*(\\d+)', flags=re.IGNORECASE)\n",
    "\n",
    "        def _extract_mass_num(title):\n",
    "            if not isinstance(title, str):\n",
    "                return None\n",
    "            m = pattern.match(title)\n",
    "            if m:\n",
    "                try:\n",
    "                    return int(m.group(1))\n",
    "                except ValueError:\n",
    "                    return None\n",
    "            return None\n",
    "\n",
    "        df[\"__mass_num\"] = df[\"comment_title\"].apply(_extract_mass_num)\n",
    "\n",
    "        # mask for mass rows\n",
    "        mask_mass = df[\"__mass_num\"].notna()\n",
    "\n",
    "        # create mass_count column\n",
    "        df[\"mass_count\"] = 0\n",
    "\n",
    "        if mask_mass.any():\n",
    "            # if docket column exists -- counts per (__mass_num, docket_id)\n",
    "            if \"docket_id\" in df.columns:\n",
    "                # (mass_num, docket_id) -> count\n",
    "                counts = df.loc[mask_mass].groupby([\"__mass_num\", \"docket_id\"]).size()\n",
    "                # assign mass_count\n",
    "                def _lookup_count(row):\n",
    "                    key = (row[\"__mass_num\"], row[\"docket_id\"])\n",
    "                    return int(counts.get(key, 0))\n",
    "                df.loc[mask_mass, \"mass_count\"] = df.loc[mask_mass].apply(_lookup_count, axis=1)\n",
    "            else:\n",
    "                # counts per mass_num across whole df if docket unspecified \n",
    "                counts = df.loc[mask_mass].groupby(\"__mass_num\").size().to_dict()\n",
    "                df.loc[mask_mass, \"mass_count\"] = df.loc[mask_mass, \"__mass_num\"].map(lambda x: int(counts.get(x, 0)))\n",
    "\n",
    "        # keep first occurrence for each mass_num\n",
    "        before_len = len(df)\n",
    "        duplicated_mask = df.loc[mask_mass, \"__mass_num\"].duplicated(keep=\"first\")\n",
    "        dup_index = df.loc[mask_mass].index[duplicated_mask]\n",
    "        if len(dup_index) > 0:\n",
    "            df = df.drop(index=dup_index).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.reset_index(drop=True)\n",
    "        after_len = len(df)\n",
    "        print(f\"Dropped {before_len - after_len} duplicate 'Mass Comment N' rows (kept first of each N).\")\n",
    "\n",
    "        # drop helper column\n",
    "        df = df.drop(columns=\"__mass_num\")\n",
    "\n",
    "        # explode & access attachment text:\n",
    "        a = df.explode('comment_text_sources')\n",
    "        b = a['comment_text_sources'].apply(pd.Series)\n",
    "        df = pd.concat([a.drop(columns='comment_text_sources'), b], axis=1)\n",
    "\n",
    "        # handle cases where attachments have identical text -- i.e. a pdf/docx submission of the same comment\n",
    "        df['text_clean'] = df['text'].apply(lambda x: str(x).lower().strip())\n",
    "        cluster = pd.DataFrame(df['text_clean'].value_counts())\n",
    "        df = df.merge(cluster, how='left', on='text_clean')\n",
    "        df = df.drop_duplicates(subset=['comment_tracking_nbr', 'comment_title', 'text_clean', 'count'])\n",
    "        df = df.drop(columns=['text_clean', 'count'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def filter_by_docket(df: pd.DataFrame, docket: str | None) -> pd.DataFrame:\n",
    "    if docket is None:\n",
    "        return df\n",
    "    if \"docket_id\" not in df.columns:\n",
    "        raise ValueError(\"docket_id column not in dataframe\")\n",
    "    df_sub = df[df[\"docket_id\"] == docket].reset_index(drop=True)\n",
    "    print(f\"Filtered to docket '{docket}': {len(df_sub)} comments\")\n",
    "    return df_sub\n",
    "\n",
    "def train_bertopic(documents: List[str], embedding_model_name: str, verbose: bool = True) -> Tuple[BERTopic, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns (model, embeddings)\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Loading embedding model:\", embedding_model_name)\n",
    "    embedder = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "    # compute embeddings (NN)\n",
    "    if verbose:\n",
    "        print(\"Computing embeddings for\", len(documents), \"documents...\")\n",
    "    embeddings = embedder.encode(documents, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "    # UMAP (dimension reduction of embeddings) and HDBSCAN instances (clustering algo used on embedding representation of comments)\n",
    "    umap_model = umap.UMAP(n_neighbors=UMAP_N_NEIGHBORS, n_components=UMAP_N_COMPONENTS, min_dist=UMAP_MIN_DIST, metric=\"cosine\", random_state=42)\n",
    "    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=HDBSCAN_MIN_CLUSTER_SIZE, min_samples=HDBSCAN_MIN_SAMPLES, metric=\"euclidean\", cluster_selection_method=\"eom\", prediction_data=True)\n",
    "\n",
    "    # instantiate BERTopic with our reducers/clusters\n",
    "    topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model, calculate_probabilities=True, verbose=verbose)\n",
    "    if verbose:\n",
    "        print(\"Training BERTopic...\")\n",
    "    topics, probs = topic_model.fit_transform(documents, embeddings)\n",
    "    if verbose:\n",
    "        print(\"BERTopic training complete. Generated\", len(set(topics)) - (1 if -1 in topics else 0), \"non-outlier topics (excludes -1).\")\n",
    "    return topic_model, embeddings\n",
    "\n",
    "def map_dominant_and_topN_bertopic(model: BERTopic, documents: List[str], df: pd.DataFrame,\n",
    "                                   doc_id_col: str, embeddings: np.ndarray | None = None,\n",
    "                                   topN: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds:\n",
    "      - bertopic_dominant_topic\n",
    "      - bertopic_top_topics (list of best-represented topics in comment)\n",
    "      - bertopic_topic_rank_{i}\n",
    "    Pass 'embeddings' (precomputed) to avoid BERTopic trying to re-embed.\n",
    "    \"\"\"\n",
    "    # failsafe if (for some reason) we did not already compute/pass embeddings:\n",
    "    if embeddings is not None:\n",
    "        topics, probs = model.transform(documents, embeddings=embeddings)\n",
    "    else:\n",
    "        topics, probs = model.transform(documents)\n",
    "\n",
    "    df[\"bertopic_dominant_topic\"] = topics\n",
    "\n",
    "    # compute topN from probs if available\n",
    "    if probs is not None:\n",
    "        probs_arr = np.array(probs)\n",
    "        if probs_arr.ndim == 2:\n",
    "            top_indices = np.argsort(probs_arr, axis=1)[:, ::-1][:, :topN]\n",
    "            top_lists = top_indices.tolist()\n",
    "            topic_info = model.get_topic_info().reset_index(drop=True)\n",
    "            topic_ids_order = topic_info[\"Topic\"].tolist()\n",
    "            idx_to_topic = {i: tid for i, tid in enumerate(topic_ids_order)}\n",
    "            top_topics = [[idx_to_topic.get(i, -1) for i in lst] for lst in top_lists]\n",
    "            df[\"bertopic_top_topics\"] = top_topics\n",
    "            for i in range(topN):\n",
    "                df[f\"bertopic_topic_rank_{i+1}\"] = df[\"bertopic_top_topics\"].apply(lambda l: l[i] if i < len(l) else -1)\n",
    "        else:\n",
    "            df[\"bertopic_top_topics\"] = df[\"bertopic_dominant_topic\"].apply(lambda x: [int(x)] + [-1]*(topN-1))\n",
    "            for i in range(topN):\n",
    "                df[f\"bertopic_topic_rank_{i+1}\"] = df[\"bertopic_top_topics\"].apply(lambda l: l[i] if i < len(l) else -1)\n",
    "    else:\n",
    "        df[\"bertopic_top_topics\"] = df[\"bertopic_dominant_topic\"].apply(lambda x: [int(x)] + [-1]*(topN-1))\n",
    "        for i in range(topN):\n",
    "            df[f\"bertopic_topic_rank_{i+1}\"] = df[\"bertopic_top_topics\"].apply(lambda l: l[i] if i < len(l) else -1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_topic_summary_bertopic(model: BERTopic, df: pd.DataFrame, documents: List[str],\n",
    "                                 doc_id_col: str, top_words: int = 10, sample_docs: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a topic summary df for output with columns: topic_num, size, top_words, sample_comments\n",
    "    \"\"\"\n",
    "    info = model.get_topic_info()  # df with 'topic', 'count' and 'name'\n",
    "    rows = []\n",
    "    for _, row in info.iterrows():\n",
    "        tnum = int(row[\"Topic\"])\n",
    "        size = int(row[\"Count\"])\n",
    "\n",
    "        # get top words for topic (BERTopic returns list of (word, score))\n",
    "        topic_words = model.get_topic(tnum)\n",
    "        if topic_words:\n",
    "            words = [w for w, s in topic_words][:top_words]\n",
    "        else:\n",
    "            words = []\n",
    "\n",
    "        # get representative comments\n",
    "        rep_docs_list = []\n",
    "        try:\n",
    "            rep = model.get_representative_docs(tnum)\n",
    "            # handle multiple output configurations\n",
    "            if rep is None:\n",
    "                rep_docs_list = []\n",
    "            elif isinstance(rep, (list, tuple)):\n",
    "                rep_docs_list = list(rep)[:sample_docs]\n",
    "            else:\n",
    "                try:\n",
    "                    rep_docs_list = list(rep)[:sample_docs]\n",
    "                except Exception:\n",
    "                    rep_docs_list = []\n",
    "        except Exception:\n",
    "            rep_docs_list = []\n",
    "\n",
    "        # if no representative docs from model grab df rows for that topic instead\n",
    "        if not rep_docs_list:\n",
    "            try:\n",
    "                mask = df[\"bertopic_dominant_topic\"] == tnum\n",
    "                rep_docs_list = df.loc[mask, TEXT_COL].astype(str).tolist()[:sample_docs]\n",
    "            except Exception:\n",
    "                rep_docs_list = []\n",
    "\n",
    "        # format sample comments\n",
    "        sample_texts = []\n",
    "        for s in rep_docs_list:\n",
    "            try:\n",
    "                s_str = str(s).replace(\"\\n\", \" \")\n",
    "            except Exception:\n",
    "                s_str = \"\"\n",
    "            sample_texts.append(s_str[:400])\n",
    "\n",
    "        rows.append({\n",
    "            \"topic_num\": tnum,\n",
    "            \"size\": size,\n",
    "            \"top_words\": \", \".join(words),\n",
    "            \"sample_comments\": \" ||| \".join(sample_texts)\n",
    "        })\n",
    "\n",
    "    summary_df = pd.DataFrame(rows).sort_values(\"size\", ascending=False).reset_index(drop=True)\n",
    "    return summary_df\n",
    "\n",
    "def chunk_documents(documents: List[str], chunk_size: int = 80, overlap: int = 20):\n",
    "    chunks = []\n",
    "    origin_ids = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        toks = str(doc).split()\n",
    "        if len(toks) <= chunk_size:\n",
    "            chunks.append(\" \".join(toks))\n",
    "            origin_ids.append(i)\n",
    "        else:\n",
    "            start = 0\n",
    "            while start < len(toks):\n",
    "                end = min(len(toks), start + chunk_size)\n",
    "                chunks.append(\" \".join(toks[start:end]))\n",
    "                origin_ids.append(i)\n",
    "                if end == len(toks):\n",
    "                    break\n",
    "                start = end - overlap\n",
    "    return chunks, origin_ids\n",
    "\n",
    "def train_bertopic_on_chunks(documents: List[str], embedding_model_name: str,\n",
    "                             chunk_size=80, overlap=20,\n",
    "                             umap_n_neighbors=10, umap_min_dist=0.0, umap_n_components=5, cluster_selection_epsilon=0.2,\n",
    "                             hdb_min_cluster_size=3, hdb_min_samples=1,\n",
    "                             verbose=True):\n",
    "    # chunk\n",
    "    chunks, origin_ids = chunk_documents(documents, chunk_size=chunk_size, overlap=overlap)\n",
    "    print(f\"Created {len(chunks)} chunks from {len(documents)} documents (ratio {len(chunks)/len(documents):.2f}).\")\n",
    "\n",
    "    # embed\n",
    "    embedder = SentenceTransformer(embedding_model_name)\n",
    "    embeddings = embedder.encode(chunks, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "    # build UMAP/HDBSCAN\n",
    "    umap_model = umap.UMAP(n_neighbors=umap_n_neighbors, n_components=umap_n_components,\n",
    "                           min_dist=umap_min_dist, metric=\"cosine\", random_state=42)\n",
    "    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=hdb_min_cluster_size, cluster_selection_epsilon=HDBSCAN_CLUSTER_SELECTION_EPSILON,\n",
    "                                    min_samples=hdb_min_samples,\n",
    "                                    metric=\"euclidean\", cluster_selection_method=\"eom\",\n",
    "                                    prediction_data=True)\n",
    "    topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model,\n",
    "                           calculate_probabilities=True, verbose=verbose)\n",
    "\n",
    "    # fit\n",
    "    topics, probs = topic_model.fit_transform(chunks, embeddings)\n",
    "    print(\"BERTopic (chunks) training complete. Non-outlier topics:\",\n",
    "          len(set(topics)) - (1 if -1 in topics else 0))\n",
    "    return topic_model, chunks, origin_ids, embeddings, topics, probs\n",
    "\n",
    "def aggregate_chunk_topics_to_docs(chunks, origin_ids, chunk_topics, chunk_probs, df, doc_id_col, topN=3):\n",
    "    \"\"\"\n",
    "    Aggregates chunk-level topics back to doc-level.\n",
    "    - Majority vote on chunk topics per document for dominant topic.\n",
    "    - For topN, uses chunk counts per topic (you can extend to use probabilities).\n",
    "    \"\"\"\n",
    "    tmp = _pd.DataFrame({\n",
    "        \"origin_idx\": origin_ids,\n",
    "        \"chunk_topic\": chunk_topics\n",
    "    })\n",
    "    # count topic frequency per original doc index\n",
    "    counts = tmp.groupby([\"origin_idx\", \"chunk_topic\"]).size().rename(\"cnt\").reset_index()\n",
    "    # find dominant topic per origin_idx\n",
    "    dominant = counts.sort_values([\"origin_idx\", \"cnt\"], ascending=[True, False]).groupby(\"origin_idx\").first().reset_index()\n",
    "    dominant_map = dict(zip(dominant[\"origin_idx\"], dominant[\"chunk_topic\"]))\n",
    "\n",
    "    # build topN lists per origin_idx\n",
    "    topn_map = {}\n",
    "    for oid, group in counts.groupby(\"origin_idx\"):\n",
    "        top_topics = group.sort_values(\"cnt\", ascending=False).head(topN)[\"chunk_topic\"].astype(int).tolist()\n",
    "        # pad if needed\n",
    "        while len(top_topics) < topN:\n",
    "            top_topics.append(-1)\n",
    "        topn_map[oid] = top_topics\n",
    "\n",
    "    # attach to df by original index\n",
    "    # original index in df corresponds to 0..len(df)-1 as created in main flow\n",
    "    df = df.copy()\n",
    "    df[\"bertopic_dominant_topic\"] = df.index.map(lambda i: int(dominant_map.get(i, -1)))\n",
    "    df[\"bertopic_top_topics\"] = df.index.map(lambda i: topn_map.get(i, [-1]*topN))\n",
    "    # expand top rank columns\n",
    "    for i in range(topN):\n",
    "        df[f\"bertopic_topic_rank_{i+1}\"] = df[\"bertopic_top_topics\"].apply(lambda l: int(l[i]) if i < len(l) else -1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0847b4a8-17ae-4dc6-93e9-572a47c69e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# parameters #\n",
    "##############\n",
    "\n",
    "CHUNK_DOCS = True              \n",
    "CHUNK_SIZE = 80                # tokens per chunk (words)\n",
    "CHUNK_OVERLAP = 20             # overlapping tokens between chunks\n",
    "HDBSCAN_MIN_CLUSTER_SIZE = 3   # 3-5 usually serves well, start tuning here. Also adjust UMAP_MIN_DISTANCE & N_COMPONENTS\n",
    "HDBSCAN_MIN_SAMPLES = 1\n",
    "HDBSCAN_CLUSTER_SELECTION_EPSILON = 0.2 # for data with meaningful large and small clusters \n",
    "UMAP_N_NEIGHBORS = 10\n",
    "UMAP_MIN_DIST = 0.07 # min dist to maintain bt points when reducing embeddings\n",
    "UMAP_N_COMPONENTS = 20 # n-dimensions\n",
    "EMBEDDING_MODEL = \"all-mpnet-base-v2\" # pretrained NN\n",
    "\n",
    "TOP_WORDS_PER_TOPIC = 7\n",
    "SAMPLE_DOCS_PER_TOPIC = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "746d7798-f9a7-4650-941a-19ae74983890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, deduplicate attachments and mass comments\n",
    "# df = load_data(DATA_PATH)\n",
    "# df = filter_by_docket(df, DOCKET_TO_USE)\n",
    "# df = df.dropna(subset=[TEXT_COL]).reset_index(drop=True)\n",
    "# docs = df[TEXT_COL].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdd76ddb-3781-4a83-a58a-a2696d13a41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 8545 duplicate 'Mass Comment N' rows (kept first of each N).\n",
      "Filtered to docket 'TTB-2025-0002': 262 comments\n",
      "Created 627 chunks from 262 documents (ratio 2.39).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328e54cf63c04ed39e9b1eab92cff471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 15:34:18,409 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-12-17 15:35:07,404 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-12-17 15:35:07,411 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-12-17 15:35:09,121 - BERTopic - Cluster - Completed ✓\n",
      "2025-12-17 15:35:09,139 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-12-17 15:35:10,035 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTopic (chunks) training complete. Non-outlier topics: 87\n"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "# implement topic modeling pipeline with chunking #\n",
    "###################################################\n",
    "\n",
    "# load, deduplicate (attachments and mass comments)\n",
    "df = load_data(DATA_PATH)\n",
    "df = filter_by_docket(df, DOCKET_TO_USE)\n",
    "df = df.dropna(subset=[TEXT_COL]).reset_index(drop=True)\n",
    "docs = df[TEXT_COL].astype(str).tolist()\n",
    "\n",
    "# embeddings, dimension reduction, clustering\n",
    "topic_model, chunks, origin_ids, embeddings, topics, probs = train_bertopic_on_chunks(\n",
    "    docs,\n",
    "    EMBEDDING_MODEL,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    overlap=CHUNK_OVERLAP,\n",
    "    umap_n_neighbors=UMAP_N_NEIGHBORS,\n",
    "    umap_min_dist=UMAP_MIN_DIST,\n",
    "    umap_n_components=UMAP_N_COMPONENTS,\n",
    "    hdb_min_cluster_size=HDBSCAN_MIN_CLUSTER_SIZE,\n",
    "    hdb_min_samples=HDBSCAN_MIN_SAMPLES,\n",
    "    cluster_selection_epsilon=HDBSCAN_CLUSTER_SELECTION_EPSILON,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1f95b20-de5b-4a65-9a27-7dd3b211138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # always an option to consolidate -- usually somewhere between 20-30 works well\n",
    "# topic_model.reduce_topics(chunks, nr_topics=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2883497-c781-4ed0-8bad-aff58b8a4525",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# aggregate chunk-level topics back to original df rows\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_with_topics \u001b[38;5;241m=\u001b[39m \u001b[43maggregate_chunk_topics_to_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDOC_ID_COL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# build topic summary (prev step ensures example comments, not chunks)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m topic_summary \u001b[38;5;241m=\u001b[39m build_topic_summary_bertopic(topic_model, df_with_topics, chunks, DOC_ID_COL, top_words\u001b[38;5;241m=\u001b[39mTOP_WORDS_PER_TOPIC, sample_docs\u001b[38;5;241m=\u001b[39mSAMPLE_DOCS_PER_TOPIC)\n",
      "Cell \u001b[1;32mIn[4], line 270\u001b[0m, in \u001b[0;36maggregate_chunk_topics_to_docs\u001b[1;34m(chunks, origin_ids, chunk_topics, chunk_probs, df, doc_id_col, topN)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maggregate_chunk_topics_to_docs\u001b[39m(chunks, origin_ids, chunk_topics, chunk_probs, df, doc_id_col, topN\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m    265\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;124;03m    Aggregates chunk-level topics back to doc-level.\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;124;03m    - Majority vote on chunk topics per document for dominant topic.\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;124;03m    - For topN, uses chunk counts per topic (you can extend to use probabilities).\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[43m_pd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morigin_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m: origin_ids,\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_topic\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk_topics\n\u001b[0;32m    273\u001b[0m     })\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;66;03m# count topic frequency per original doc index\u001b[39;00m\n\u001b[0;32m    275\u001b[0m     counts \u001b[38;5;241m=\u001b[39m tmp\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morigin_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_topic\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;241m.\u001b[39mrename(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "\u001b[1;31mNameError\u001b[0m: name '_pd' is not defined"
     ]
    }
   ],
   "source": [
    "# aggregate chunk-level topics back to original df rows\n",
    "df_with_topics = aggregate_chunk_topics_to_docs(chunks, origin_ids, topics, probs, df, DOC_ID_COL, topN=3)\n",
    "\n",
    "# build topic summary (prev step ensures example comments, not chunks)\n",
    "topic_summary = build_topic_summary_bertopic(topic_model, df_with_topics, chunks, DOC_ID_COL, top_words=TOP_WORDS_PER_TOPIC, sample_docs=SAMPLE_DOCS_PER_TOPIC)\n",
    "\n",
    "SAVE_NAME = OUTPUTS_DIR / f\"bertopic_topic_summary_{DOCKET_TO_USE}.csv\"\n",
    "OUTPUT_NAME = OUTPUTS_DIR / f\"comments_with_bertopic_{DOCKET_TO_USE}.csv\"\n",
    "\n",
    "# save outputs\n",
    "topic_summary.to_csv(SAVE_NAME, index=False)\n",
    "df_with_topics.to_csv(OUTPUT_NAME, index=False)\n",
    "# topic_model.save(str(MODEL_SAVE_FILE))\n",
    "print(\"Saved outputs. Topics:\", len(topic_model.get_topic_info()) - (1 if -1 in topic_model.get_topic_info()[\"Topic\"].tolist() else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eefae0-8654-4907-bccf-55cd147a4df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example implementation without chunking:\n",
    "\n",
    "# df = load_data(DATA_PATH)\n",
    "# df = filter_by_docket(df, DOCKET_TO_USE)\n",
    "# df = df.dropna(subset=[TEXT_COL]).reset_index(drop=True)\n",
    "# docs = df[TEXT_COL].astype(str).tolist()\n",
    "\n",
    "# topic_model, embeddings = train_bertopic(\n",
    "#     docs,\n",
    "#     EMBEDDING_MODEL,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# df = map_dominant_and_topN_bertopic(model=topic_model, documents=docs, df=df, doc_id_col=DOC_ID_COL, embeddings=embeddings)\n",
    "\n",
    "# df_sum = build_topic_summary_bertopic(topic_model, df, docs, DOC_ID_COL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
