{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1707e041-8c68-43f2-9190-a8c83c7affae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52f50b12-fdf5-43a4-a57a-1aa084c5b2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# parameters #\n",
    "##############\n",
    "\n",
    "DATA_PATH = Path(r\"C:\\Users\\linna\\OneDrive\\Documents\\Python_Dev\\topic-modeling\\data\\public_comments.json\")\n",
    "\n",
    "TEXT_COL = \"comment_text\"\n",
    "DOC_ID_COL = \"comment_id\"\n",
    "DOCKET_TO_USE = \"TTB-2025-0003\"\n",
    "\n",
    "# repo / outputs\n",
    "try:\n",
    "    REPO_ROOT = Path(__file__).parent.parent.resolve()\n",
    "except NameError:\n",
    "    REPO_ROOT = Path(os.getcwd()).parent.resolve()\n",
    "\n",
    "OUTPUTS_DIR = REPO_ROOT / \"outputs\"\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TOPIC_SUMMARY_CSV = OUTPUTS_DIR / \"bertopic_topic_summary.csv\"\n",
    "OUTPUT_DF_CSV = OUTPUTS_DIR / \"comments_with_bertopic.csv\"\n",
    "MODEL_SAVE_FILE = Path(r\"C:\\Users\\linna\\Documents\\bertopic_model\")\n",
    "\n",
    "# for later dev -- want to remove nonsense tokens AFTER topics are computed:\n",
    "# STOPWORDS = set(stopwords.words('english'))\n",
    "# BAD_DISPLAY_TOKENS = {\"/sbr/s\", \"brand39s\", str(STOPWORDS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36463ac5-75b0-41eb-8216-d68174e52180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_json(path, orient=\"records\", lines=False)\n",
    "    if TEXT_COL not in df.columns:\n",
    "        raise ValueError(f\"{TEXT_COL} not found in dataframe columns: {df.columns.tolist()}\")\n",
    "    df = df.dropna(subset=[TEXT_COL]).reset_index(drop=True)\n",
    "    print(f\"Loaded {len(df)} comments from {path}\")\n",
    "    return df\n",
    "\n",
    "def filter_by_docket(df: pd.DataFrame, docket: str | None) -> pd.DataFrame:\n",
    "    if docket is None:\n",
    "        return df\n",
    "    if \"docket_id\" not in df.columns:\n",
    "        raise ValueError(\"docket_id column not in dataframe\")\n",
    "    df_sub = df[df[\"docket_id\"] == docket].reset_index(drop=True)\n",
    "    print(f\"Filtered to docket '{docket}': {len(df_sub)} comments\")\n",
    "    return df_sub\n",
    "\n",
    "def chunk_documents(documents: List[str], chunk_size: int = 200, overlap: int = 50) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Simple sliding-window chunker returning (chunks, chunk_origin_doc_id)\n",
    "    chunk_size = number of tokens (approx via split) per chunk\n",
    "    overlap = overlapping tokens between chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    origin_ids = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        toks = str(doc).split()\n",
    "        if len(toks) <= chunk_size:\n",
    "            chunks.append(\" \".join(toks))\n",
    "            origin_ids.append(i)\n",
    "        else:\n",
    "            start = 0\n",
    "            while start < len(toks):\n",
    "                end = min(len(toks), start + chunk_size)\n",
    "                chunks.append(\" \".join(toks[start:end]))\n",
    "                origin_ids.append(i)\n",
    "                if end == len(toks):\n",
    "                    break\n",
    "                start = end - overlap\n",
    "    return chunks, origin_ids\n",
    "\n",
    "def train_bertopic(documents: List[str], embedding_model_name: str, verbose: bool = True) -> Tuple[BERTopic, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns (model, embeddings)\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Loading embedding model:\", embedding_model_name)\n",
    "    embedder = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "    # compute embeddings first (we can reuse them)\n",
    "    if verbose:\n",
    "        print(\"Computing embeddings for\", len(documents), \"documents...\")\n",
    "    embeddings = embedder.encode(documents, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "    # build UMAP and HDBSCAN instances with tunable params\n",
    "    umap_model = umap.UMAP(n_neighbors=UMAP_N_NEIGHBORS, n_components=UMAP_N_COMPONENTS, min_dist=UMAP_MIN_DIST, metric=\"cosine\", random_state=42)\n",
    "    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=HDBSCAN_MIN_CLUSTER_SIZE, min_samples=HDBSCAN_MIN_SAMPLES, metric=\"euclidean\", cluster_selection_method=\"eom\", prediction_data=True)\n",
    "\n",
    "    # instantiate BERTopic with our reducers/clusters\n",
    "    topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model, calculate_probabilities=True, verbose=verbose)\n",
    "    if verbose:\n",
    "        print(\"Training BERTopic...\")\n",
    "    topics, probs = topic_model.fit_transform(documents, embeddings)\n",
    "    if verbose:\n",
    "        print(\"BERTopic training complete. Generated\", len(set(topics)) - (1 if -1 in topics else 0), \"non-outlier topics (excludes -1).\")\n",
    "    return topic_model, embeddings\n",
    "\n",
    "def map_dominant_and_topN_bertopic(model: BERTopic, documents: List[str], df: pd.DataFrame,\n",
    "                                   doc_id_col: str, embeddings: np.ndarray | None = None,\n",
    "                                   topN: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds:\n",
    "      - bertopic_dominant_topic\n",
    "      - bertopic_top_topics (list of best-represented topics in comment)\n",
    "      - bertopic_topic_rank_{i}\n",
    "    Pass 'embeddings' (precomputed) to avoid BERTopic trying to re-embed.\n",
    "    \"\"\"\n",
    "    # failsafe if (for some reason) we did not already compute/pass embeddings:\n",
    "    if embeddings is not None:\n",
    "        topics, probs = model.transform(documents, embeddings=embeddings)\n",
    "    else:\n",
    "        topics, probs = model.transform(documents)\n",
    "\n",
    "    df[\"bertopic_dominant_topic\"] = topics\n",
    "\n",
    "    # compute topN from probs if available\n",
    "    if probs is not None:\n",
    "        probs_arr = np.array(probs)\n",
    "        if probs_arr.ndim == 2:\n",
    "            top_indices = np.argsort(probs_arr, axis=1)[:, ::-1][:, :topN]\n",
    "            top_lists = top_indices.tolist()\n",
    "            topic_info = model.get_topic_info().reset_index(drop=True)\n",
    "            topic_ids_order = topic_info[\"Topic\"].tolist()\n",
    "            idx_to_topic = {i: tid for i, tid in enumerate(topic_ids_order)}\n",
    "            top_topics = [[idx_to_topic.get(i, -1) for i in lst] for lst in top_lists]\n",
    "            df[\"bertopic_top_topics\"] = top_topics\n",
    "            for i in range(topN):\n",
    "                df[f\"bertopic_topic_rank_{i+1}\"] = df[\"bertopic_top_topics\"].apply(lambda l: l[i] if i < len(l) else -1)\n",
    "        else:\n",
    "            df[\"bertopic_top_topics\"] = df[\"bertopic_dominant_topic\"].apply(lambda x: [int(x)] + [-1]*(topN-1))\n",
    "            for i in range(topN):\n",
    "                df[f\"bertopic_topic_rank_{i+1}\"] = df[\"bertopic_top_topics\"].apply(lambda l: l[i] if i < len(l) else -1)\n",
    "    else:\n",
    "        df[\"bertopic_top_topics\"] = df[\"bertopic_dominant_topic\"].apply(lambda x: [int(x)] + [-1]*(topN-1))\n",
    "        for i in range(topN):\n",
    "            df[f\"bertopic_topic_rank_{i+1}\"] = df[\"bertopic_top_topics\"].apply(lambda l: l[i] if i < len(l) else -1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_topic_summary_bertopic(model: BERTopic, df: pd.DataFrame, documents: List[str],\n",
    "                                 doc_id_col: str, top_words: int = 10, sample_docs: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a topic summary df for output with columns: topic_num, size, top_words, sample_comments\n",
    "    \"\"\"\n",
    "    info = model.get_topic_info()  # df with 'topic', 'count' and 'name'\n",
    "    rows = []\n",
    "    for _, row in info.iterrows():\n",
    "        tnum = int(row[\"Topic\"])\n",
    "        size = int(row[\"Count\"])\n",
    "\n",
    "        # get top words for topic (BERTopic returns list of (word, score))\n",
    "        topic_words = model.get_topic(tnum)\n",
    "        if topic_words:\n",
    "            words = [w for w, s in topic_words][:top_words]\n",
    "        else:\n",
    "            words = []\n",
    "\n",
    "        # get representative comments\n",
    "        rep_docs_list = []\n",
    "        try:\n",
    "            rep = model.get_representative_docs(tnum)\n",
    "            # handle multiple output configurations\n",
    "            if rep is None:\n",
    "                rep_docs_list = []\n",
    "            elif isinstance(rep, (list, tuple)):\n",
    "                rep_docs_list = list(rep)[:sample_docs]\n",
    "            else:\n",
    "                try:\n",
    "                    rep_docs_list = list(rep)[:sample_docs]\n",
    "                except Exception:\n",
    "                    rep_docs_list = []\n",
    "        except Exception:\n",
    "            rep_docs_list = []\n",
    "\n",
    "        # if no representative docs from model grab df rows for that topic instead\n",
    "        if not rep_docs_list:\n",
    "            try:\n",
    "                mask = df[\"bertopic_dominant_topic\"] == tnum\n",
    "                rep_docs_list = df.loc[mask, TEXT_COL].astype(str).tolist()[:sample_docs]\n",
    "            except Exception:\n",
    "                rep_docs_list = []\n",
    "\n",
    "        # format sample comments\n",
    "        sample_texts = []\n",
    "        for s in rep_docs_list:\n",
    "            try:\n",
    "                s_str = str(s).replace(\"\\n\", \" \")\n",
    "            except Exception:\n",
    "                s_str = \"\"\n",
    "            sample_texts.append(s_str[:400])\n",
    "\n",
    "        rows.append({\n",
    "            \"topic_num\": tnum,\n",
    "            \"size\": size,\n",
    "            \"top_words\": \", \".join(words),\n",
    "            \"sample_comments\": \" ||| \".join(sample_texts)\n",
    "        })\n",
    "\n",
    "    summary_df = pd.DataFrame(rows).sort_values(\"size\", ascending=False).reset_index(drop=True)\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0847b4a8-17ae-4dc6-93e9-572a47c69e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_DOCS = True              \n",
    "CHUNK_SIZE = 80                # tokens per chunk (words)\n",
    "CHUNK_OVERLAP = 20             # overlapping tokens between chunks\n",
    "HDBSCAN_MIN_CLUSTER_SIZE = 3   # 3-5 usually serves well, start tuning here. Also adjust UMAP_MIN_DISTANCE & N_COMPONENTS\n",
    "HDBSCAN_MIN_SAMPLES = 1\n",
    "UMAP_N_NEIGHBORS = 10\n",
    "UMAP_MIN_DIST = 0.05\n",
    "UMAP_N_COMPONENTS = 5\n",
    "EMBEDDING_MODEL = \"all-mpnet-base-v2\"\n",
    "\n",
    "TOP_WORDS_PER_TOPIC = 7\n",
    "SAMPLE_DOCS_PER_TOPIC = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0cb0ed-e003-46dd-aada-153268f3dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_documents(documents: List[str], chunk_size: int = 80, overlap: int = 20):\n",
    "    chunks = []\n",
    "    origin_ids = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        toks = str(doc).split()\n",
    "        if len(toks) <= chunk_size:\n",
    "            chunks.append(\" \".join(toks))\n",
    "            origin_ids.append(i)\n",
    "        else:\n",
    "            start = 0\n",
    "            while start < len(toks):\n",
    "                end = min(len(toks), start + chunk_size)\n",
    "                chunks.append(\" \".join(toks[start:end]))\n",
    "                origin_ids.append(i)\n",
    "                if end == len(toks):\n",
    "                    break\n",
    "                start = end - overlap\n",
    "    return chunks, origin_ids\n",
    "\n",
    "def train_bertopic_on_chunks(documents: List[str], embedding_model_name: str,\n",
    "                             chunk_size=80, overlap=20,\n",
    "                             umap_n_neighbors=10, umap_min_dist=0.0, umap_n_components=5,\n",
    "                             hdb_min_cluster_size=3, hdb_min_samples=1,\n",
    "                             verbose=True):\n",
    "    # chunk\n",
    "    chunks, origin_ids = chunk_documents(documents, chunk_size=chunk_size, overlap=overlap)\n",
    "    print(f\"Created {len(chunks)} chunks from {len(documents)} documents (ratio {len(chunks)/len(documents):.2f}).\")\n",
    "\n",
    "    # embed\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    embedder = SentenceTransformer(embedding_model_name)\n",
    "    embeddings = embedder.encode(chunks, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "    # build UMAP/HDBSCAN\n",
    "    import umap\n",
    "    import hdbscan\n",
    "    umap_model = umap.UMAP(n_neighbors=umap_n_neighbors, n_components=umap_n_components,\n",
    "                           min_dist=umap_min_dist, metric=\"cosine\", random_state=42)\n",
    "    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=hdb_min_cluster_size,\n",
    "                                    min_samples=hdb_min_samples,\n",
    "                                    metric=\"euclidean\", cluster_selection_method=\"eom\",\n",
    "                                    prediction_data=True)\n",
    "    from bertopic import BERTopic\n",
    "    topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model,\n",
    "                           calculate_probabilities=True, verbose=verbose)\n",
    "\n",
    "    # fit\n",
    "    topics, probs = topic_model.fit_transform(chunks, embeddings)\n",
    "    print(\"BERTopic (chunks) training complete. Non-outlier topics:\",\n",
    "          len(set(topics)) - (1 if -1 in topics else 0))\n",
    "    return topic_model, chunks, origin_ids, embeddings, topics, probs\n",
    "\n",
    "def aggregate_chunk_topics_to_docs(chunks, origin_ids, chunk_topics, chunk_probs, df, doc_id_col, topN=3):\n",
    "    \"\"\"\n",
    "    Aggregates chunk-level topics back to doc-level.\n",
    "    - Majority vote on chunk topics per document for dominant topic.\n",
    "    - For topN, uses chunk counts per topic (you can extend to use probabilities).\n",
    "    \"\"\"\n",
    "    import pandas as _pd\n",
    "    tmp = _pd.DataFrame({\n",
    "        \"origin_idx\": origin_ids,\n",
    "        \"chunk_topic\": chunk_topics\n",
    "    })\n",
    "    # count topic frequency per original doc index\n",
    "    counts = tmp.groupby([\"origin_idx\", \"chunk_topic\"]).size().rename(\"cnt\").reset_index()\n",
    "    # find dominant topic per origin_idx\n",
    "    dominant = counts.sort_values([\"origin_idx\", \"cnt\"], ascending=[True, False]).groupby(\"origin_idx\").first().reset_index()\n",
    "    dominant_map = dict(zip(dominant[\"origin_idx\"], dominant[\"chunk_topic\"]))\n",
    "\n",
    "    # build topN lists per origin_idx\n",
    "    topn_map = {}\n",
    "    for oid, group in counts.groupby(\"origin_idx\"):\n",
    "        top_topics = group.sort_values(\"cnt\", ascending=False).head(topN)[\"chunk_topic\"].astype(int).tolist()\n",
    "        # pad if needed\n",
    "        while len(top_topics) < topN:\n",
    "            top_topics.append(-1)\n",
    "        topn_map[oid] = top_topics\n",
    "\n",
    "    # attach to df by original index\n",
    "    # original index in df corresponds to 0..len(df)-1 as created in main flow\n",
    "    df = df.copy()\n",
    "    df[\"bertopic_dominant_topic\"] = df.index.map(lambda i: int(dominant_map.get(i, -1)))\n",
    "    df[\"bertopic_top_topics\"] = df.index.map(lambda i: topn_map.get(i, [-1]*topN))\n",
    "    # expand top rank columns\n",
    "    for i in range(topN):\n",
    "        df[f\"bertopic_topic_rank_{i+1}\"] = df[\"bertopic_top_topics\"].apply(lambda l: int(l[i]) if i < len(l) else -1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdd76ddb-3781-4a83-a58a-a2696d13a41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12437 comments from C:\\Users\\linna\\OneDrive\\Documents\\Python_Dev\\topic-modeling\\data\\public_comments.json\n",
      "Filtered to docket 'TTB-2025-0003': 189 comments\n",
      "Created 407 chunks from 189 documents (ratio 2.15).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85f9c0187d14c23a615ebf8c20b778e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 14:19:08,503 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-08 14:19:40,528 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-08 14:19:40,534 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-08 14:19:41,123 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-08 14:19:41,141 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-09-08 14:19:41,623 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTopic (chunks) training complete. Non-outlier topics: 62\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Example run (replace with your main flow)\n",
    "# ----------------------------\n",
    "df = load_data(DATA_PATH)\n",
    "df = filter_by_docket(df, DOCKET_TO_USE)\n",
    "df = df.dropna(subset=[TEXT_COL]).reset_index(drop=True)\n",
    "docs = df[TEXT_COL].astype(str).tolist()\n",
    "\n",
    "topic_model, chunks, origin_ids, embeddings, topics, probs = train_bertopic_on_chunks(\n",
    "    docs,\n",
    "    EMBEDDING_MODEL,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    overlap=CHUNK_OVERLAP,\n",
    "    umap_n_neighbors=UMAP_N_NEIGHBORS,\n",
    "    umap_min_dist=UMAP_MIN_DIST,\n",
    "    umap_n_components=UMAP_N_COMPONENTS,\n",
    "    hdb_min_cluster_size=HDBSCAN_MIN_CLUSTER_SIZE,\n",
    "    hdb_min_samples=HDBSCAN_MIN_SAMPLES,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2883497-c781-4ed0-8bad-aff58b8a4525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 14:19:42,994 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved outputs. Topics: 62\n"
     ]
    }
   ],
   "source": [
    "# aggregate chunk-level topics back to original df rows\n",
    "df_with_topics = aggregate_chunk_topics_to_docs(chunks, origin_ids, topics, probs, df, DOC_ID_COL, topN=3)\n",
    "\n",
    "# build topic summary (you can reuse your build_topic_summary_bertopic function,\n",
    "# but when chunking, representative docs will be chunk-level; prefer sampling from original docs)\n",
    "topic_summary = build_topic_summary_bertopic(topic_model, df_with_topics, chunks, DOC_ID_COL, top_words=TOP_WORDS_PER_TOPIC, sample_docs=SAMPLE_DOCS_PER_TOPIC)\n",
    "\n",
    "# save outputs\n",
    "topic_summary.to_csv(TOPIC_SUMMARY_CSV, index=False)\n",
    "df_with_topics.to_csv(OUTPUT_DF_CSV, index=False)\n",
    "topic_model.save(str(MODEL_SAVE_FILE))\n",
    "print(\"Saved outputs. Topics:\", len(topic_model.get_topic_info()) - (1 if -1 in topic_model.get_topic_info()[\"Topic\"].tolist() else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1f95b20-de5b-4a65-9a27-7dd3b211138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# always an option to consolidate -- usually somewhere between 20-30 works well\n",
    "# topic_model.reduce_topics(chunks, nr_topics=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
