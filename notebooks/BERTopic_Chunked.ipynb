{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1707e041-8c68-43f2-9190-a8c83c7affae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# imports #\n",
    "###########\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f50b12-fdf5-43a4-a57a-1aa084c5b2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# parameters #\n",
    "##############\n",
    "\n",
    "DATA_PATH = Path(r\"C:\\Users\\linna\\OneDrive\\Documents\\Python_Dev\\topic-modeling\\data\\comments_09DEC2025.json\")\n",
    "\n",
    "TEXT_COL = \"text\"\n",
    "DOC_ID_COL = \"comment_id\"\n",
    "DOCKET_TO_USE = \"TTB-2025-0002\"\n",
    "\n",
    "# repo / outputs\n",
    "try:\n",
    "    REPO_ROOT = Path(__file__).parent.parent.resolve()\n",
    "except NameError:\n",
    "    REPO_ROOT = Path(os.getcwd()).parent.resolve()\n",
    "\n",
    "OUTPUTS_DIR = REPO_ROOT / \"outputs\"\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TOPIC_SUMMARY_CSV = OUTPUTS_DIR / \"bertopic_topic_summary.csv\"\n",
    "OUTPUT_DF_CSV = OUTPUTS_DIR / \"comments_with_bertopic.csv\"\n",
    "MODEL_SAVE_FILE = Path(r\"C:\\Users\\linna\\Documents\\bertopic_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36463ac5-75b0-41eb-8216-d68174e52180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# functions #\n",
    "#############\n",
    "\n",
    "def load_data(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_json(path, orient=\"records\", lines=False)\n",
    "\n",
    "    # explode & access attachment text:\n",
    "    a = df.explode('comment_text_sources')\n",
    "    b = a['comment_text_sources'].apply(pd.Series)\n",
    "    df = pd.concat([a.drop(columns='comment_text_sources'), b], axis=1)\n",
    "    \n",
    "    if TEXT_COL not in df.columns:\n",
    "        raise ValueError(f\"{TEXT_COL} not found in dataframe columns: {df.columns.tolist()}\")\n",
    "\n",
    "    if \"comment_title\" in df.columns:\n",
    "        # deduplicate mass comments, compute docket-level counts\n",
    "        pattern = re.compile(r'^\\s*Mass Comment\\s*[#\\(\\-:\\s]*\\s*(\\d+)', flags=re.IGNORECASE)\n",
    "\n",
    "        def _extract_mass_num(title):\n",
    "            if not isinstance(title, str):\n",
    "                return None\n",
    "            m = pattern.match(title)\n",
    "            if m:\n",
    "                try:\n",
    "                    return int(m.group(1))\n",
    "                except ValueError:\n",
    "                    return None\n",
    "            return None\n",
    "\n",
    "        df[\"__mass_num\"] = df[\"comment_title\"].apply(_extract_mass_num)\n",
    "\n",
    "        # mask for mass rows\n",
    "        mask_mass = df[\"__mass_num\"].notna()\n",
    "\n",
    "        # create mass_count column\n",
    "        df[\"mass_count\"] = 0\n",
    "\n",
    "        if mask_mass.any():\n",
    "            # if docket column exists -- counts per (__mass_num, docket_id)\n",
    "            if \"docket_id\" in df.columns:\n",
    "                # (mass_num, docket_id) -> count\n",
    "                counts = df.loc[mask_mass].groupby([\"__mass_num\", \"docket_id\"]).size()\n",
    "                # assign mass_count\n",
    "                def _lookup_count(row):\n",
    "                    key = (row[\"__mass_num\"], row[\"docket_id\"])\n",
    "                    return int(counts.get(key, 0))\n",
    "                df.loc[mask_mass, \"mass_count\"] = df.loc[mask_mass].apply(_lookup_count, axis=1)\n",
    "            else:\n",
    "                # counts per mass_num across whole df if docket unspecified \n",
    "                counts = df.loc[mask_mass].groupby(\"__mass_num\").size().to_dict()\n",
    "                df.loc[mask_mass, \"mass_count\"] = df.loc[mask_mass, \"__mass_num\"].map(lambda x: int(counts.get(x, 0)))\n",
    "\n",
    "        # keep first occurrence for each mass_num\n",
    "        before_len = len(df)\n",
    "        duplicated_mask = df.loc[mask_mass, \"__mass_num\"].duplicated(keep=\"first\")\n",
    "        dup_index = df.loc[mask_mass].index[duplicated_mask]\n",
    "        if len(dup_index) > 0:\n",
    "            df = df.drop(index=dup_index).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.reset_index(drop=True)\n",
    "        after_len = len(df)\n",
    "        print(f\"Dropped {before_len - after_len} duplicate 'Mass Comment N' rows (kept first of each N).\")\n",
    "\n",
    "        # drop helper column\n",
    "        df = df.drop(columns=\"__mass_num\")\n",
    "\n",
    "        # # explode & access attachment text:\n",
    "        # a = df.explode('comment_text_sources')\n",
    "        # b = a['comment_text_sources'].apply(pd.Series)\n",
    "        # df = pd.concat([a.drop(columns='comment_text_sources'), b], axis=1)\n",
    "\n",
    "        # handle cases where attachments have identical text -- i.e. a pdf/docx submission of the same comment\n",
    "        df['text_clean'] = df['text'].apply(lambda x: str(x).lower().strip())\n",
    "        cluster = pd.DataFrame(df['text_clean'].value_counts())\n",
    "        df = df.merge(cluster, how='left', on='text_clean')\n",
    "        df = df.drop_duplicates(subset=['comment_tracking_nbr', 'comment_title', 'text_clean', 'count'])\n",
    "        df = df.drop(columns=['text_clean', 'count'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def filter_by_docket(df: pd.DataFrame, docket: str | None) -> pd.DataFrame:\n",
    "    if docket is None:\n",
    "        return df\n",
    "    if \"docket_id\" not in df.columns:\n",
    "        raise ValueError(\"docket_id column not in dataframe\")\n",
    "    df_sub = df[df[\"docket_id\"] == docket].reset_index(drop=True)\n",
    "    print(f\"Filtered to docket '{docket}': {len(df_sub)} comments\")\n",
    "    return df_sub\n",
    "\n",
    "def train_bertopic(documents: List[str], embedding_model_name: str, verbose: bool = True) -> Tuple[BERTopic, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns (model, embeddings)\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Loading embedding model:\", embedding_model_name)\n",
    "    embedder = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "    # compute embeddings (NN)\n",
    "    if verbose:\n",
    "        print(\"Computing embeddings for\", len(documents), \"documents...\")\n",
    "    embeddings = embedder.encode(documents, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "    # UMAP (dimension reduction of embeddings) and HDBSCAN instances (clustering algo used on embedding representation of comments)\n",
    "    umap_model = umap.UMAP(n_neighbors=UMAP_N_NEIGHBORS, n_components=UMAP_N_COMPONENTS, min_dist=UMAP_MIN_DIST, metric=\"cosine\", random_state=42)\n",
    "    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=HDBSCAN_MIN_CLUSTER_SIZE, min_samples=HDBSCAN_MIN_SAMPLES, metric=\"euclidean\", cluster_selection_method=\"eom\", prediction_data=True)\n",
    "\n",
    "    # instantiate BERTopic with our reducers/clusters\n",
    "    topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model, calculate_probabilities=True, verbose=verbose)\n",
    "    if verbose:\n",
    "        print(\"Training BERTopic...\")\n",
    "    topics, probs = topic_model.fit_transform(documents, embeddings)\n",
    "    if verbose:\n",
    "        print(\"BERTopic training complete. Generated\", len(set(topics)) - (1 if -1 in topics else 0), \"non-outlier topics (excludes -1).\")\n",
    "    return topic_model, embeddings\n",
    "\n",
    "def map_dominant_and_topN_bertopic(model: BERTopic, documents: List[str], df: pd.DataFrame,\n",
    "                                   doc_id_col: str, embeddings: np.ndarray | None = None,\n",
    "                                   topN: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds:\n",
    "      - bertopic_dominant_topic\n",
    "      - bertopic_top_topics (list of best-represented topics in comment)\n",
    "      - bertopic_topic_rank_{i}\n",
    "    Pass 'embeddings' (precomputed) to avoid BERTopic trying to re-embed.\n",
    "    \"\"\"\n",
    "    # failsafe if (for some reason) we did not already compute/pass embeddings:\n",
    "    if embeddings is not None:\n",
    "        topics, probs = model.transform(documents, embeddings=embeddings)\n",
    "    else:\n",
    "        topics, probs = model.transform(documents)\n",
    "\n",
    "    df[\"bertopic_dominant_topic\"] = topics\n",
    "\n",
    "    # compute topN from probs if available\n",
    "    if probs is not None:\n",
    "        probs_arr = np.array(probs)\n",
    "        if probs_arr.ndim == 2:\n",
    "            top_indices = np.argsort(probs_arr, axis=1)[:, ::-1][:, :topN]\n",
    "            top_lists = top_indices.tolist()\n",
    "            topic_info = model.get_topic_info().reset_index(drop=True)\n",
    "            topic_ids_order = topic_info[\"Topic\"].tolist()\n",
    "            idx_to_topic = {i: tid for i, tid in enumerate(topic_ids_order)}\n",
    "            top_topics = [[idx_to_topic.get(i, -1) for i in lst] for lst in top_lists]\n",
    "            df[\"bertopic_top_topics\"] = top_topics\n",
    "            for i in range(topN):\n",
    "                df[f\"bertopic_topic_rank_{i+1}\"] = df[\"bertopic_top_topics\"].apply(lambda l: l[i] if i < len(l) else -1)\n",
    "        else:\n",
    "            df[\"bertopic_top_topics\"] = df[\"bertopic_dominant_topic\"].apply(lambda x: [int(x)] + [-1]*(topN-1))\n",
    "            for i in range(topN):\n",
    "                df[f\"bertopic_topic_rank_{i+1}\"] = df[\"bertopic_top_topics\"].apply(lambda l: l[i] if i < len(l) else -1)\n",
    "    else:\n",
    "        df[\"bertopic_top_topics\"] = df[\"bertopic_dominant_topic\"].apply(lambda x: [int(x)] + [-1]*(topN-1))\n",
    "        for i in range(topN):\n",
    "            df[f\"bertopic_topic_rank_{i+1}\"] = df[\"bertopic_top_topics\"].apply(lambda l: l[i] if i < len(l) else -1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_topic_summary_bertopic(model: BERTopic, df: pd.DataFrame, documents: List[str],\n",
    "                                 doc_id_col: str, top_words: int = 10, sample_docs: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a topic summary df for output with columns: topic_num, size, top_words, sample_comments\n",
    "    \"\"\"\n",
    "    comments = df.copy()\n",
    "    info = model.get_topic_info()  # df with 'topic', 'count' and 'name'\n",
    "    rows = []\n",
    "    for _, row in info.iterrows():\n",
    "        tnum = int(row[\"Topic\"])\n",
    "\n",
    "        # get top words for topic (BERTopic returns list of (word, score))\n",
    "        topic_words = model.get_topic(tnum)\n",
    "        if topic_words:\n",
    "            words = [w for w, s in topic_words][:top_words]\n",
    "        else:\n",
    "            words = []\n",
    "\n",
    "        # get representative comments\n",
    "        rep_docs_list = []\n",
    "        try:\n",
    "            rep = model.get_representative_docs(tnum)\n",
    "            # handle multiple output configurations\n",
    "            if rep is None:\n",
    "                rep_docs_list = []\n",
    "            elif isinstance(rep, (list, tuple)):\n",
    "                rep_docs_list = list(rep)[:sample_docs]\n",
    "            else:\n",
    "                try:\n",
    "                    rep_docs_list = list(rep)[:sample_docs]\n",
    "                except Exception:\n",
    "                    rep_docs_list = []\n",
    "        except Exception:\n",
    "            rep_docs_list = []\n",
    "\n",
    "        # if no representative docs from model grab df rows for that topic instead\n",
    "        if not rep_docs_list:\n",
    "            try:\n",
    "                mask = df[\"bertopic_dominant_topic\"] == tnum\n",
    "                rep_docs_list = df.loc[mask, TEXT_COL].astype(str).tolist()[:sample_docs]\n",
    "            except Exception:\n",
    "                rep_docs_list = []\n",
    "\n",
    "        # format sample comments\n",
    "        sample_texts = []\n",
    "        for s in rep_docs_list:\n",
    "            try:\n",
    "                s_str = str(s).replace(\"\\n\", \" \")\n",
    "            except Exception:\n",
    "                s_str = \"\"\n",
    "            sample_texts.append(s_str[:400])\n",
    "\n",
    "        rows.append({\n",
    "            \"topic_num\": tnum,\n",
    "            \"top_words\": \", \".join(words),\n",
    "            \"sample_comments\": \" ||| \".join(sample_texts)\n",
    "        })\n",
    "\n",
    "    summary_df = pd.DataFrame(rows)\n",
    "    counts = pd.DataFrame(comments[\"bertopic_dominant_topic\"].value_counts()).reset_index()\n",
    "    summary_df = summary_df.merge(counts, how='left', left_on='topic_num', right_on='bertopic_dominant_topic')\n",
    "    summary_df = summary_df.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "    summary_df = summary_df.rename(columns={\"count\":\"size\"})\n",
    "    summary_df = summary_df.dropna(subset=['size'])\n",
    "    summary_df['size'] = summary_df['size'].astype(int)\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "def chunk_documents(documents: List[str], chunk_size: int = 80, overlap: int = 20):\n",
    "    chunks = []\n",
    "    origin_ids = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        toks = str(doc).split()\n",
    "        if len(toks) <= chunk_size:\n",
    "            chunks.append(\" \".join(toks))\n",
    "            origin_ids.append(i)\n",
    "        else:\n",
    "            start = 0\n",
    "            while start < len(toks):\n",
    "                end = min(len(toks), start + chunk_size)\n",
    "                chunks.append(\" \".join(toks[start:end]))\n",
    "                origin_ids.append(i)\n",
    "                if end == len(toks):\n",
    "                    break\n",
    "                start = end - overlap\n",
    "    return chunks, origin_ids\n",
    "\n",
    "def train_bertopic_on_chunks(documents: List[str], embedding_model_name: str,\n",
    "                             chunk_size=80, overlap=20,\n",
    "                             umap_n_neighbors=10, umap_min_dist=0.0, umap_n_components=5, cluster_selection_epsilon=0.2, cluster_selection_method=\"eom\",\n",
    "                             hdb_min_cluster_size=3, hdb_min_samples=1,\n",
    "                             verbose=True):\n",
    "    # chunk\n",
    "    chunks, origin_ids = chunk_documents(documents, chunk_size=chunk_size, overlap=overlap)\n",
    "    print(f\"Created {len(chunks)} chunks from {len(documents)} documents (ratio {len(chunks)/len(documents):.2f}).\")\n",
    "\n",
    "    # embed\n",
    "    embedder = SentenceTransformer(embedding_model_name)\n",
    "    embeddings = embedder.encode(chunks, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "    # build UMAP/HDBSCAN\n",
    "    umap_model = umap.UMAP(n_neighbors=umap_n_neighbors, n_components=umap_n_components,\n",
    "                           min_dist=umap_min_dist, metric=\"cosine\", random_state=42)\n",
    "    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=hdb_min_cluster_size, cluster_selection_epsilon=cluster_selection_epsilon,\n",
    "                                    min_samples=hdb_min_samples,\n",
    "                                    metric=\"euclidean\", cluster_selection_method=cluster_selection_method,\n",
    "                                    prediction_data=True)\n",
    "    topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model,\n",
    "                           calculate_probabilities=True, verbose=verbose)\n",
    "\n",
    "    # fit\n",
    "    topics, probs = topic_model.fit_transform(chunks, embeddings)\n",
    "    print(\"BERTopic (chunks) training complete. Non-outlier topics:\",\n",
    "          len(set(topics)) - (1 if -1 in topics else 0))\n",
    "    return topic_model, chunks, origin_ids, embeddings, topics, probs\n",
    "\n",
    "def aggregate_chunk_topics_to_docs(chunks, origin_ids, chunk_topics, chunk_probs, df, doc_id_col, topN=3):\n",
    "    \n",
    "    tmp = pd.DataFrame({\n",
    "        \"origin_idx\": origin_ids,\n",
    "        \"chunk_topic\": chunk_topics\n",
    "    })\n",
    "    # count topic frequency per original doc index\n",
    "    counts = tmp.groupby([\"origin_idx\", \"chunk_topic\"]).size().rename(\"cnt\").reset_index()\n",
    "    # find dominant topic per origin_idx\n",
    "    dominant = counts.sort_values([\"origin_idx\", \"cnt\"], ascending=[True, False]).groupby(\"origin_idx\").first().reset_index()\n",
    "    dominant_map = dict(zip(dominant[\"origin_idx\"], dominant[\"chunk_topic\"]))\n",
    "\n",
    "    # build topN lists per origin_idx\n",
    "    topn_map = {}\n",
    "    for oid, group in counts.groupby(\"origin_idx\"):\n",
    "        top_topics = group.sort_values(\"cnt\", ascending=False).head(topN)[\"chunk_topic\"].tolist()#astype(int).tolist()\n",
    "        # pad if needed\n",
    "        while len(top_topics) < topN:\n",
    "            top_topics.append(-1)\n",
    "        topn_map[oid] = top_topics\n",
    "\n",
    "    # attach to df by original index\n",
    "    # original index in df corresponds to 0..len(df)-1 as created in main flow\n",
    "    df = df.copy()\n",
    "    df[\"bertopic_dominant_topic\"] = df.index.map(lambda i: int(dominant_map.get(i, -1)))\n",
    "    df[\"bertopic_top_topics\"] = df.index.map(lambda i: topn_map.get(i, [-1]*topN))\n",
    "    # expand top rank columns\n",
    "    for i in range(topN):\n",
    "        df[f\"bertopic_topic_rank_{i+1}\"] = df[\"bertopic_top_topics\"].apply(lambda l: int(l[i]) if i < len(l) else -1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0847b4a8-17ae-4dc6-93e9-572a47c69e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# parameters #\n",
    "##############\n",
    "\n",
    "CHUNK_DOCS = True              \n",
    "CHUNK_SIZE = 300                # tokens per chunk (words)\n",
    "CHUNK_OVERLAP = 20             # overlapping tokens between chunks\n",
    "HDBSCAN_MIN_CLUSTER_SIZE = 4   # 3-5 usually serves well, start tuning here. Also adjust UMAP_MIN_DISTANCE & N_COMPONENTS\n",
    "HDBSCAN_MIN_SAMPLES = 1\n",
    "HDBSCAN_CLUSTER_SELECTION_METHOD = \"leaf\" # eom is default\n",
    "HDBSCAN_CLUSTER_SELECTION_EPSILON = 0.31 # for data with meaningful large and small clusters \n",
    "UMAP_N_NEIGHBORS = 15\n",
    "UMAP_MIN_DIST = 0.07 # min dist to maintain bt points when reducing embeddings\n",
    "UMAP_N_COMPONENTS = 15 # n-dimensions\n",
    "EMBEDDING_MODEL = \"all-mpnet-base-v2\" # pretrained NN\n",
    "\n",
    "TOP_WORDS_PER_TOPIC = 7\n",
    "SAMPLE_DOCS_PER_TOPIC = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdd76ddb-3781-4a83-a58a-a2696d13a41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 8602 duplicate 'Mass Comment N' rows (kept first of each N).\n",
      "Filtered to docket 'TTB-2025-0002': 262 comments\n",
      "Created 732 chunks from 262 documents (ratio 2.79).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db762950fd7467e95e86f4f766611a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 12:46:36,910 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-12-23 12:47:17,168 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-12-23 12:47:17,169 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-12-23 12:47:18,214 - BERTopic - Cluster - Completed ✓\n",
      "2025-12-23 12:47:18,228 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-12-23 12:47:19,141 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTopic (chunks) training complete. Non-outlier topics: 43\n"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "# implement topic modeling pipeline with chunking #\n",
    "###################################################\n",
    "\n",
    "# load, deduplicate (attachments and mass comments)\n",
    "df = load_data(DATA_PATH)\n",
    "df = filter_by_docket(df, DOCKET_TO_USE)\n",
    "df = df.dropna(subset=[TEXT_COL]).reset_index(drop=True)\n",
    "docs = df[TEXT_COL].astype(str).tolist()\n",
    "\n",
    "# embeddings, dimension reduction, clustering\n",
    "topic_model, chunks, origin_ids, embeddings, topics, probs = train_bertopic_on_chunks(\n",
    "    docs,\n",
    "    EMBEDDING_MODEL,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    overlap=CHUNK_OVERLAP,\n",
    "    umap_n_neighbors=UMAP_N_NEIGHBORS,\n",
    "    umap_min_dist=UMAP_MIN_DIST,\n",
    "    umap_n_components=UMAP_N_COMPONENTS,\n",
    "    hdb_min_cluster_size=HDBSCAN_MIN_CLUSTER_SIZE,\n",
    "    hdb_min_samples=HDBSCAN_MIN_SAMPLES,\n",
    "    cluster_selection_epsilon=HDBSCAN_CLUSTER_SELECTION_EPSILON,\n",
    "    cluster_selection_method=HDBSCAN_CLUSTER_SELECTION_METHOD,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9412fca4-cca3-483e-87fd-ed12982c748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate chunk-level topics back to original df rows\n",
    "df_with_topics = aggregate_chunk_topics_to_docs(chunks, origin_ids, topics, probs, df, DOC_ID_COL, topN=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1f95b20-de5b-4a65-9a27-7dd3b211138b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # always an option to consolidate -- usually somewhere between 20-30 works well\n",
    "# topic_model = topic_model.reduce_topics(chunks, nr_topics=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a65a061-f652-43e3-bfa6-5531f72bc034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attachment_count</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>document_id</th>\n",
       "      <th>comment_type</th>\n",
       "      <th>comment_last_modified_date</th>\n",
       "      <th>comment_highlighted_content</th>\n",
       "      <th>comment_withdrawn</th>\n",
       "      <th>comment_title</th>\n",
       "      <th>comment_object_id</th>\n",
       "      <th>comment_posted_date</th>\n",
       "      <th>...</th>\n",
       "      <th>fr_vol_num</th>\n",
       "      <th>start_end_page</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>mass_count</th>\n",
       "      <th>bertopic_dominant_topic</th>\n",
       "      <th>bertopic_top_topics</th>\n",
       "      <th>bertopic_topic_rank_1</th>\n",
       "      <th>bertopic_topic_rank_2</th>\n",
       "      <th>bertopic_topic_rank_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TTB-2025-0002-0138</td>\n",
       "      <td>TTB-2025-0002-0001</td>\n",
       "      <td>Public Submission</td>\n",
       "      <td>2025-06-23 18:20:06</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>Comment 135:  Government of Japan</td>\n",
       "      <td>09000064b8e23d47</td>\n",
       "      <td>2025-06-23 04:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>90 FR 6654</td>\n",
       "      <td>6654 – 6706</td>\n",
       "      <td>DIRECT (NON-ATTACHMENT)</td>\n",
       "      <td>The attached comments are submitted by the USA...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>[15, -1, -1]</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>TTB-2025-0002-0138</td>\n",
       "      <td>TTB-2025-0002-0001</td>\n",
       "      <td>Public Submission</td>\n",
       "      <td>2025-06-23 18:20:06</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>Comment 135:  Government of Japan</td>\n",
       "      <td>09000064b8e23d47</td>\n",
       "      <td>2025-06-23 04:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>90 FR 6654</td>\n",
       "      <td>6654 – 6706</td>\n",
       "      <td>https://downloads.regulations.gov/TTB-2025-000...</td>\n",
       "      <td>\\n \\n1 \\n \\nJapan’s Comment on the “G/TBT/N/U...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>[15, 16, -1]</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   attachment_count          comment_id         document_id  \\\n",
       "0                 1  TTB-2025-0002-0138  TTB-2025-0002-0001   \n",
       "1                 1  TTB-2025-0002-0138  TTB-2025-0002-0001   \n",
       "\n",
       "        comment_type comment_last_modified_date comment_highlighted_content  \\\n",
       "0  Public Submission        2025-06-23 18:20:06                               \n",
       "1  Public Submission        2025-06-23 18:20:06                               \n",
       "\n",
       "   comment_withdrawn                      comment_title comment_object_id  \\\n",
       "0              False  Comment 135:  Government of Japan  09000064b8e23d47   \n",
       "1              False  Comment 135:  Government of Japan  09000064b8e23d47   \n",
       "\n",
       "   comment_posted_date  ...  fr_vol_num start_end_page  \\\n",
       "0  2025-06-23 04:00:00  ...  90 FR 6654    6654 – 6706   \n",
       "1  2025-06-23 04:00:00  ...  90 FR 6654    6654 – 6706   \n",
       "\n",
       "                                              source  \\\n",
       "0                            DIRECT (NON-ATTACHMENT)   \n",
       "1  https://downloads.regulations.gov/TTB-2025-000...   \n",
       "\n",
       "                                                text mass_count  \\\n",
       "0  The attached comments are submitted by the USA...          0   \n",
       "1   \\n \\n1 \\n \\nJapan’s Comment on the “G/TBT/N/U...          0   \n",
       "\n",
       "   bertopic_dominant_topic  bertopic_top_topics  bertopic_topic_rank_1  \\\n",
       "0                       15         [15, -1, -1]                     15   \n",
       "1                       15         [15, 16, -1]                     15   \n",
       "\n",
       "  bertopic_topic_rank_2 bertopic_topic_rank_3  \n",
       "0                    -1                    -1  \n",
       "1                    16                    -1  \n",
       "\n",
       "[2 rows x 52 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_topics.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b1d0620-51b9-4ad1-b7f3-c5fb22f055a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build topic summary (prev step ensures example comments, not chunks)\n",
    "topic_summary = build_topic_summary_bertopic(topic_model, df_with_topics, chunks, DOC_ID_COL, top_words=TOP_WORDS_PER_TOPIC, sample_docs=SAMPLE_DOCS_PER_TOPIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2883497-c781-4ed0-8bad-aff58b8a4525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved outputs. Topics: 43\n"
     ]
    }
   ],
   "source": [
    "# build topic summary (prev step ensures example comments, not chunks)\n",
    "topic_summary = build_topic_summary_bertopic(topic_model, df_with_topics, chunks, DOC_ID_COL, top_words=TOP_WORDS_PER_TOPIC, sample_docs=SAMPLE_DOCS_PER_TOPIC)\n",
    "\n",
    "SAVE_NAME = OUTPUTS_DIR / f\"bertopic_topic_summary_{DOCKET_TO_USE}.csv\"\n",
    "OUTPUT_NAME = OUTPUTS_DIR / f\"comments_with_bertopic_{DOCKET_TO_USE}.csv\"\n",
    "\n",
    "# save outputs\n",
    "topic_summary.to_csv(SAVE_NAME, index=False)\n",
    "df_with_topics.to_csv(OUTPUT_NAME, index=False)\n",
    "# topic_model.save(str(MODEL_SAVE_FILE))\n",
    "print(\"Saved outputs. Topics:\", len(topic_model.get_topic_info()) - (1 if -1 in topic_model.get_topic_info()[\"Topic\"].tolist() else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94eefae0-8654-4907-bccf-55cd147a4df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example implementation without chunking:\n",
    "\n",
    "# df = load_data(DATA_PATH)\n",
    "# df = filter_by_docket(df, DOCKET_TO_USE)\n",
    "# df = df.dropna(subset=[TEXT_COL]).reset_index(drop=True)\n",
    "# docs = df[TEXT_COL].astype(str).tolist()\n",
    "\n",
    "# topic_model, embeddings = train_bertopic(\n",
    "#     docs,\n",
    "#     EMBEDDING_MODEL,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# df = map_dominant_and_topN_bertopic(model=topic_model, documents=docs, df=df, doc_id_col=DOC_ID_COL, embeddings=embeddings)\n",
    "\n",
    "# df_sum = build_topic_summary_bertopic(topic_model, df, docs, DOC_ID_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea48e73-2f53-4489-83ca-79b6a364bcfc",
   "metadata": {},
   "source": [
    "*** Params for 0003 ***\n",
    "\n",
    "CHUNK_DOCS = True              \n",
    "CHUNK_SIZE = 300                # tokens per chunk (words)\n",
    "CHUNK_OVERLAP = 20             # overlapping tokens between chunks\n",
    "HDBSCAN_MIN_CLUSTER_SIZE = 4   # 3-5 usually serves well, start tuning here. Also adjust UMAP_MIN_DISTANCE & N_COMPONENTS\n",
    "HDBSCAN_MIN_SAMPLES = 1\n",
    "HDBSCAN_CLUSTER_SELECTION_METHOD = \"leaf\" # eom is default\n",
    "HDBSCAN_CLUSTER_SELECTION_EPSILON = 0.27 # for data with meaningful large and small clusters \n",
    "UMAP_N_NEIGHBORS = 15\n",
    "UMAP_MIN_DIST = 0.07 # min dist to maintain bt points when reducing embeddings\n",
    "UMAP_N_COMPONENTS = 15 # n-dimensions\n",
    "EMBEDDING_MODEL = \"all-mpnet-base-v2\" # pretrained NN\n",
    "\n",
    "TOP_WORDS_PER_TOPIC = 7\n",
    "SAMPLE_DOCS_PER_TOPIC = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9b6f6-0831-4f87-ba8c-866e96c33079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
