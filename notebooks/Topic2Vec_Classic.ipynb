{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeac4e0-ec84-4638-abe9-b5f8c8990c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn\n",
    "import scipy\n",
    "from pathlib import Path\n",
    "\n",
    "# Topic modeling with embeddings\n",
    "from top2vec import Top2Vec\n",
    "import joblib \n",
    "\n",
    "# Guided topic modeling\n",
    "from corextopic import corextopic as ct\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.sparse as ss\n",
    "\n",
    "# Sentiment analysis\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Interactive visuals\n",
    "import streamlit as st\n",
    "import plotly.express as px\n",
    "import umap\n",
    "\n",
    "# Path to JSON file (one row per comment; each row a dict/object)\n",
    "DATA_PATH = Path(\"C:\\\\Users\\\\linna\\\\OneDrive\\\\Documents\\\\Python_Dev\\\\topic-modeling\\\\data\\\\public_comments.json\")\n",
    "\n",
    "df = pd.read_json(DATA_PATH, orient=\"records\", lines=False)  # or lines=True if newline-delimited\n",
    "\n",
    "# Ensure comments exist\n",
    "TEXT_COL = \"comment_text\"     \n",
    "if TEXT_COL not in df.columns:\n",
    "    raise ValueError(f\"{TEXT_COL} not found in dataframe columns: {df.columns.tolist()}\")\n",
    "\n",
    "df = df.dropna(subset=[TEXT_COL]).reset_index(drop=True)\n",
    "\n",
    "print(\"Loaded\", len(df), \"comments\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c51fd1f-732e-45cc-b7ed-2bd99a1682f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_238 = df[df['docket_id'] == 'TTB-2025-0003']\n",
    "pr_237 = df[df['docket_id'] == 'TTB-2025-0002']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b1fe707-e8da-49a2-b996-906c5dd6a084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTB-2025-0003: 189 comments\n",
      "TTB-2025-0002: 174 comments\n"
     ]
    }
   ],
   "source": [
    "print('TTB-2025-0003: ' + str(len(pr_238)) + ' comments')\n",
    "print('TTB-2025-0002: ' + str(len(pr_237)) + ' comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "808177dd-18a6-40e0-beaf-d814ce64ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pr_238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeb06a4f-6793-4c51-8d03-1e407e8f174e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs in df: 189\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=[TEXT_COL]).reset_index(drop=True)\n",
    "print(\"Docs in df:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f3c8a33f-6748-494b-9e7e-42a8068a44ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df = pd.concat([pr_238, pr_237])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ab71fb20-f314-4beb-a717-0a8089012dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df.to_csv(\"C:\\\\Users\\\\linna\\\\OneDrive\\\\Documents\\\\Python_Dev\\\\topic-modeling\\\\data\\\\example_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83664638-00ad-4c04-aa72-a002b7ea6b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 14:10:45,572 - top2vec - INFO - Pre-processing documents for training\n",
      "C:\\Users\\linna\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "2025-08-28 14:10:45,911 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
      "2025-08-28 14:10:47,505 - top2vec - INFO - Creating joint document/word embedding\n",
      "2025-08-28 14:11:01,759 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2025-08-28 14:11:35,254 - top2vec - INFO - Finding dense areas of documents\n",
      "C:\\Users\\linna\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\linna\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "2025-08-28 14:11:35,329 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "# Prepare documents (list of comments as strings) and document (comment) ids\n",
    "documents = df[TEXT_COL].astype(str).tolist()\n",
    "# Keep comment_id as strings (Top2Vec will return these when using document_ids)\n",
    "document_ids = df[\"comment_id\"].astype(str).tolist()\n",
    "\n",
    "# Set embedding model and training parameters\n",
    "embedding_model = \"all-MiniLM-L6-v2\"\n",
    "speed = \"deep-learn\"\n",
    "workers = os.cpu_count() or 1\n",
    "\n",
    "# Train model (may take a while depending on number of comments and chosen 'speed' \n",
    "model = Top2Vec(\n",
    "    documents=documents,\n",
    "    document_ids=document_ids,\n",
    "    embedding_model=embedding_model,\n",
    "    speed=speed,\n",
    "    workers=workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01a901d0-489c-49ea-9925-c0b8caf531b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics discovered: 1\n",
      "Top 10 topic sizes (docs per topic):\n",
      "  Topic 0: 189 documents\n"
     ]
    }
   ],
   "source": [
    "# Topics\n",
    "n_topics = model.get_num_topics()\n",
    "print(\"Number of topics discovered:\", n_topics)\n",
    "\n",
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print(\"Top 10 topic sizes (docs per topic):\")\n",
    "for size, num in zip(topic_sizes[:10], topic_nums[:10]):\n",
    "    print(f\"  Topic {num}: {size} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7d8479c-96d2-4eb6-b031-2a792fb7b142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0\n",
      "  allergens (0.553)\n",
      "  allergies (0.401)\n",
      "  label (0.356)\n",
      "  labeling (0.336)\n",
      "  alcohol (0.335)\n",
      "  allergen (0.315)\n",
      "  beverages (0.315)\n",
      "  labels (0.304)\n",
      "  ingredients (0.303)\n",
      "  alcoholic (0.262)\n"
     ]
    }
   ],
   "source": [
    "# Top N words for top N topics\n",
    "top_n = min(10, n_topics)\n",
    "topic_words, word_scores, topic_numbers = model.get_topics(top_n)\n",
    "for words, scores, tnum in zip(topic_words, word_scores, topic_numbers):\n",
    "    print(\"\\nTopic\", tnum)\n",
    "    # show top 10 words (they come ordered)\n",
    "    for w, s in zip(words[:10], scores[:10]):\n",
    "        print(f\"  {w} ({s:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06c0ddc9-efe5-4654-8835-b7f61ae8370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map dominant topic back to df\n",
    "# Use search_documents_by_topic for each topic and assign docs to their best topic.\n",
    "docid_to_topic = {}\n",
    "topic_sizes, topic_nums = model.get_topic_sizes()  # topics ordered by size\n",
    "\n",
    "for size, tnum in zip(topic_sizes, topic_nums):\n",
    "    # retrieve all documents for this topic (num_docs = topic size)\n",
    "    docs, doc_scores, doc_ids = model.search_documents_by_topic(topic_num=tnum, num_docs=int(size))\n",
    "    # doc_ids are the original document_ids you passed to the model\n",
    "    for did in doc_ids:\n",
    "        # assign topic tnum as the dominant topic for did\n",
    "        # if a doc appears in multiple lists (unlikely), first assignment wins (largest topic first)\n",
    "        if did not in docid_to_topic:\n",
    "            docid_to_topic[did] = tnum\n",
    "\n",
    "# Create a column in df with the dominant topic (or -1 if not assigned)\n",
    "df[\"top2vec_dominant_topic\"] = df[\"comment_id\"].astype(str).map(docid_to_topic).fillna(-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d23bcf48-3c5b-4e19-af32-5c5659f98fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label with top terms\n",
    "top_n = 5\n",
    "\n",
    "# Get the words for every topic the model discovered\n",
    "num_topics = model.get_num_topics()\n",
    "topic_words, word_scores, topic_nums = model.get_topics(num_topics)\n",
    "\n",
    "topic_label_map = {}\n",
    "for tnum, words in zip(topic_nums, topic_words):\n",
    "    words_sel = words[:top_n]                     \n",
    "    label = \", \".join(words_sel)                  \n",
    "    topic_label_map[int(tnum)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "143e9465-1854-4854-95ae-102b122f8d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map to df; fallback label for unknown topics\n",
    "df[\"top2vec_terms\"] = (\n",
    "    df[\"top2vec_dominant_topic\"]\n",
    "      .map(topic_label_map)\n",
    "      .fillna(\"Unclear\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "632cf0b4-c922-42eb-bb70-49c6bdd817e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: C:\\Users\\linna\\OneDrive\\Documents\\Python_Dev\\topic-modeling\\models\\top2vec_model\n"
     ]
    }
   ],
   "source": [
    "# Save Topic2Vec model (Top2Vec has its own save/load)\n",
    "model_save_path = \"C:\\\\Users\\\\linna\\\\OneDrive\\\\Documents\\\\Python_Dev\\\\topic-modeling\\\\models\\\\top2vec_model\"\n",
    "model.save(model_save_path)\n",
    "print(\"Model saved to:\", model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
